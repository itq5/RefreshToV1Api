# å¯¼å…¥æ‰€éœ€çš„åº“
import base64
import hashlib
import json
import logging
import mimetypes
import os
import requests
import uuid
from datetime import datetime
from fake_useragent import UserAgent
from flask import Flask, request, jsonify, Response, send_from_directory
from flask_apscheduler import APScheduler
from flask_cors import CORS, cross_origin
from io import BytesIO
from logging.handlers import TimedRotatingFileHandler
from queue import Queue
from urllib.parse import urlparse


# è¯»å–é…ç½®æ–‡ä»¶
def load_config(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        return json.load(file)


CONFIG = load_config('./data/config.json')

LOG_LEVEL = CONFIG.get('log_level', 'INFO').upper()
NEED_LOG_TO_FILE = CONFIG.get('need_log_to_file', 'true').lower() == 'true'

# ä½¿ç”¨ get æ–¹æ³•è·å–é…ç½®é¡¹ï¼ŒåŒæ—¶æä¾›é»˜è®¤å€¼
BASE_URL = CONFIG.get('upstream_base_url', '')
PROXY_API_PREFIX = CONFIG.get('upstream_api_prefix', '')

UPLOAD_BASE_URL = CONFIG.get('backend_container_url', '')
KEY_FOR_GPTS_INFO = CONFIG.get('key_for_gpts_info', '')
KEY_FOR_GPTS_INFO_ACCESS_TOKEN = CONFIG.get('key_for_gpts_info', '')
API_PREFIX = CONFIG.get('backend_container_api_prefix', '')
GPT_4_S_New_Names = CONFIG.get('gpt_4_s_new_name', 'gpt-4-s').split(',')
GPT_4_MOBILE_NEW_NAMES = CONFIG.get('gpt_4_mobile_new_name', 'gpt-4-mobile').split(',')
GPT_3_5_NEW_NAMES = CONFIG.get('gpt_3_5_new_name', 'gpt-3.5-turbo').split(',')
GPT_4_O_NEW_NAMES = CONFIG.get('gpt_4_o_new_name', 'gpt-4o').split(',')
GPT_4_O_MINI_NEW_NAMES = CONFIG.get('gpt_4_o_mini_new_name', 'gpt-4o-mini').split(',')
O1_PREVIEW_NEW_NAMES = CONFIG.get('o1_preview_new_name', 'o1-preview').split(',')
O1_MINI_NEW_NAMES = CONFIG.get('o1_mini_new_name', 'o1-mini').split(',')
UPLOAD_SUCCESS_TEXT = CONFIG.get('upload_success_text', "`ğŸ¤– æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œæœç´¢å°†ä¸å†æä¾›é¢å¤–ä¿¡æ¯ï¼`\n")

BOT_MODE = CONFIG.get('bot_mode', {})
BOT_MODE_ENABLED = BOT_MODE.get('enabled', 'false').lower() == 'true'
BOT_MODE_ENABLED_MARKDOWN_IMAGE_OUTPUT = BOT_MODE.get('enabled_markdown_image_output', 'false').lower() == 'true'
BOT_MODE_ENABLED_BING_REFERENCE_OUTPUT = BOT_MODE.get('enabled_bing_reference_output', 'false').lower() == 'true'
BOT_MODE_ENABLED_CODE_BLOCK_OUTPUT = BOT_MODE.get('enabled_plugin_output', 'false').lower() == 'true'

BOT_MODE_ENABLED_PLAIN_IMAGE_URL_OUTPUT = BOT_MODE.get('enabled_plain_image_url_output', 'false').lower() == 'true'

# oaiFreeToV1Api_refresh
REFRESH_TOACCESS = CONFIG.get('refresh_ToAccess', {})
REFRESH_TOACCESS_ENABLEOAI = REFRESH_TOACCESS.get('enableOai', 'true').lower() == 'true'
REFRESH_TOACCESS_OAIFREE_REFRESHTOACCESS_URL = REFRESH_TOACCESS.get('oaifree_refreshToAccess_Url', '')
STEAM_SLEEP_TIME = REFRESH_TOACCESS.get('steam_sleep_time', 0)

NEED_DELETE_CONVERSATION_AFTER_RESPONSE = CONFIG.get('need_delete_conversation_after_response',
                                                     'true').lower() == 'true'

USE_OAIUSERCONTENT_URL = CONFIG.get('use_oaiusercontent_url', 'false').lower() == 'true'

# USE_PANDORA_FILE_SERVER = CONFIG.get('use_pandora_file_server', 'false').lower() == 'true'

CUSTOM_ARKOSE = CONFIG.get('custom_arkose_url', 'false').lower() == 'true'

ARKOSE_URLS = CONFIG.get('arkose_urls', "")

DALLE_PROMPT_PREFIX = CONFIG.get('dalle_prompt_prefix', '')

# redisé…ç½®è¯»å–
REDIS_CONFIG = CONFIG.get('redis', {})
REDIS_CONFIG_HOST = REDIS_CONFIG.get('host', 'redis')
REDIS_CONFIG_PORT = REDIS_CONFIG.get('port', 6379)
REDIS_CONFIG_PASSWORD = REDIS_CONFIG.get('password', '')
REDIS_CONFIG_DB = REDIS_CONFIG.get('db', 0)
REDIS_CONFIG_POOL_SIZE = REDIS_CONFIG.get('pool_size', 10)
REDIS_CONFIG_POOL_TIMEOUT = REDIS_CONFIG.get('pool_timeout', 30)

# å®šä¹‰å…¨éƒ¨å˜é‡ï¼Œç”¨äºç¼“å­˜refresh_tokenå’Œaccess_token
# å…¶ä¸­refresh_token ä¸º key
# access_token ä¸º value
refresh_dict = {}

# è®¾ç½®æ—¥å¿—çº§åˆ«
log_level_dict = {
    'DEBUG': logging.DEBUG,
    'INFO': logging.INFO,
    'WARNING': logging.WARNING,
    'ERROR': logging.ERROR,
    'CRITICAL': logging.CRITICAL
}

log_formatter = logging.Formatter('%(asctime)s [%(levelname)s] - %(message)s')

logger = logging.getLogger()
logger.setLevel(log_level_dict.get(LOG_LEVEL, logging.DEBUG))

import redis

# å‡è®¾æ‚¨å·²ç»æœ‰ä¸€ä¸ªRediså®¢æˆ·ç«¯çš„å®ä¾‹
redis_client = redis.StrictRedis(host=REDIS_CONFIG_HOST,
                                 port=REDIS_CONFIG_PORT,
                                 password=REDIS_CONFIG_PASSWORD,
                                 db=REDIS_CONFIG_DB,
                                 retry_on_timeout=True
                                 )

# å¦‚æœç¯å¢ƒå˜é‡æŒ‡ç¤ºéœ€è¦è¾“å‡ºåˆ°æ–‡ä»¶
if NEED_LOG_TO_FILE:
    log_filename = './log/access.log'
    file_handler = TimedRotatingFileHandler(log_filename, when="midnight", interval=1, backupCount=30)
    file_handler.setFormatter(log_formatter)
    logger.addHandler(file_handler)

# æ·»åŠ æ ‡å‡†è¾“å‡ºæµå¤„ç†å™¨ï¼ˆæ§åˆ¶å°è¾“å‡ºï¼‰
stream_handler = logging.StreamHandler()
stream_handler.setFormatter(log_formatter)
logger.addHandler(stream_handler)

# åˆ›å»ºFakeUserAgentå¯¹è±¡
ua = UserAgent()

import threading

#  å¼€å¯çº¿ç¨‹é”
lock = threading.Lock()


def getPROXY_API_PREFIX(lock):
    index = 0
    while True:
        with lock:
            if not PROXY_API_PREFIX:
                return None
            else:
                return "/" + (PROXY_API_PREFIX[index % len(PROXY_API_PREFIX)])


def generate_unique_id(prefix):
    # ç”Ÿæˆä¸€ä¸ªéšæœºçš„ UUID
    random_uuid = uuid.uuid4()
    # å°† UUID è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œå¹¶ç§»é™¤å…¶ä¸­çš„çŸ­æ¨ªçº¿
    random_uuid_str = str(random_uuid).replace('-', '')
    # ç»“åˆå‰ç¼€å’Œå¤„ç†è¿‡çš„ UUID ç”Ÿæˆæœ€ç»ˆçš„å”¯ä¸€ ID
    unique_id = f"{prefix}-{random_uuid_str}"
    return unique_id


def get_accessible_model_list():
    return [config['name'] for config in gpts_configurations]


def find_model_config(model_name):
    for config in gpts_configurations:
        if config['name'] == model_name:
            return config
    return None


# ä» gpts.json è¯»å–é…ç½®
def load_gpts_config(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        return json.load(file)


# å®˜æ–¹refresh_tokenåˆ·æ–°access_token
def oaiGetAccessToken(refresh_token):
    logger.info("å°†é€šè¿‡è¿™ä¸ªç½‘å€è¯·æ±‚access_tokenï¼šhttps://auth0.openai.com/oauth/token")
    url = "https://auth0.openai.com/oauth/token"
    headers = {
        "Content-Type": "application/json"
    }
    data = {
        "redirect_uri": "com.openai.chat://auth0.openai.com/ios/com.openai.chat/callback",
        "grant_type": "refresh_token",
        "client_id": "pdlLIX2Y72MIl2rhLhTE9VV9bN905kBh",
        "refresh_token": refresh_token
    }
    try:
        response = requests.post(url, headers=headers, json=data)
        # å¦‚æœå“åº”çš„çŠ¶æ€ç ä¸æ˜¯ 200ï¼Œå°†å¼•å‘ HTTPError å¼‚å¸¸
        response.raise_for_status()

        # æ‹¿åˆ°access_token
        json_response = response.json()
        access_token = json_response.get('access_token')

        # æ£€æŸ¥ access_token æ˜¯å¦æœ‰æ•ˆ
        if not access_token or not access_token.startswith("eyJhb"):
            logger.error("access_token æ— æ•ˆ.")
            return None

        return access_token

    except requests.HTTPError as http_err:
        logger.error(f"HTTP error occurred: {http_err}")
    except Exception as err:
        logger.error(f"Other error occurred: {err}")
    return None


# oaiFreeè·å¾—access_token
def oaiFreeGetAccessToken(getAccessTokenUrl, refresh_token):
    try:
        logger.info("å°†é€šè¿‡è¿™ä¸ªç½‘å€è¯·æ±‚access_tokenï¼š" + getAccessTokenUrl)
        data = {
            'refresh_token': refresh_token,
        }
        response = requests.post(getAccessTokenUrl, data=data)
        if not response.ok:
            logger.error("Request å¤±è´¥: " + response.text.strip())
            return None
        access_token = None
        try:
            access_token = response.json()["access_token"]
        except json.JSONDecodeError:
            logger.exception("Failed to decode JSON response.")
        if response.status_code == 200 and access_token and access_token.startswith("eyJhb"):
            return access_token
    except Exception as e:
        logger.exception("è·å–access tokenå¤±è´¥.")
    return None


def updateGptsKey():
    global KEY_FOR_GPTS_INFO
    global KEY_FOR_GPTS_INFO_ACCESS_TOKEN
    if not KEY_FOR_GPTS_INFO == '' and not KEY_FOR_GPTS_INFO.startswith("eyJhb"):
        if REFRESH_TOACCESS_ENABLEOAI:
            access_token = oaiGetAccessToken(KEY_FOR_GPTS_INFO)
        else:
            access_token = oaiFreeGetAccessToken(REFRESH_TOACCESS_OAIFREE_REFRESHTOACCESS_URL, KEY_FOR_GPTS_INFO)
        if access_token.startswith("eyJhb"):
            KEY_FOR_GPTS_INFO_ACCESS_TOKEN = access_token
            logging.info("KEY_FOR_GPTS_INFO_ACCESS_TOKENè¢«æ›´æ–°:" + KEY_FOR_GPTS_INFO_ACCESS_TOKEN)


# æ ¹æ® ID å‘é€è¯·æ±‚å¹¶è·å–é…ç½®ä¿¡æ¯
def fetch_gizmo_info(base_url, proxy_api_prefix, model_id):
    url = f"{base_url}{proxy_api_prefix}/backend-api/gizmos/{model_id}"
    headers = {
        "Authorization": f"Bearer {KEY_FOR_GPTS_INFO_ACCESS_TOKEN}"
    }

    response = requests.get(url, headers=headers)
    # logger.debug(f"fetch_gizmo_info_response: {response.text}")
    if response.status_code == 200:
        return response.json()
    else:
        return None


# gpts_configurations = []

# å°†é…ç½®æ·»åŠ åˆ°å…¨å±€åˆ—è¡¨
def add_config_to_global_list(base_url, proxy_api_prefix, gpts_data):
    global gpts_configurations
    updateGptsKey()  # cSpell:ignore Gpts
    # print(f"gpts_data: {gpts_data}")
    for model_name, model_info in gpts_data.items():
        # print(f"model_name: {model_name}")
        # print(f"model_info: {model_info}")
        model_id = model_info['id']
        # é¦–å…ˆå°è¯•ä» Redis è·å–ç¼“å­˜æ•°æ®
        cached_gizmo_info = redis_client.get(model_id)
        if cached_gizmo_info:
            gizmo_info = eval(cached_gizmo_info)  # å°†å­—ç¬¦ä¸²è½¬æ¢å›å­—å…¸
            logger.info(f"Using cached info for {model_name}, {model_id}")
        else:
            logger.info(f"Fetching gpts info for {model_name}, {model_id}")
            gizmo_info = fetch_gizmo_info(base_url, proxy_api_prefix, model_id)
            # å¦‚æœæˆåŠŸè·å–åˆ°æ•°æ®ï¼Œåˆ™å°†å…¶å­˜å…¥ Redis
            if gizmo_info:
                redis_client.set(model_id, str(gizmo_info))
                logger.info(f"Cached gizmo info for {model_name}, {model_id}")

        # æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦å·²ç»åœ¨åˆ—è¡¨ä¸­
        if gizmo_info and not any(d['name'] == model_name for d in gpts_configurations):
            gpts_configurations.append({
                'name': model_name,
                'id': model_id,
                'config': gizmo_info
            })
        else:
            logger.info(f"Model already exists in the list, skipping...")


def generate_gpts_payload(model, messages):
    model_config = find_model_config(model)
    if model_config:
        gizmo_info = model_config['config']
        gizmo_id = gizmo_info['gizmo']['id']

        payload = {
            "action": "next",
            "messages": messages,
            "parent_message_id": str(uuid.uuid4()),
            "model": "gpt-4-gizmo",
            "timezone_offset_min": -480,
            "history_and_training_disabled": False,
            "conversation_mode": {
                "gizmo": gizmo_info,
                "kind": "gizmo_interaction",
                "gizmo_id": gizmo_id
            },
            "force_paragen": False,
            "force_rate_limit": False
        }
        return payload
    else:
        return None


# åˆ›å»º Flask åº”ç”¨
app = Flask(__name__)
CORS(app, resources={r"/images/*": {"origins": "*"}})
scheduler = APScheduler()
scheduler.init_app(app)
scheduler.start()

# PANDORA_UPLOAD_URL = 'files.pandoranext.com'


VERSION = '0.8.1'
# VERSION = 'test'
UPDATE_INFO = 'ğŸ‘€ æ”¯æŒè¾“å‡ºo1æ€è€ƒè¿‡ç¨‹'
# UPDATE_INFO = 'ã€ä»…ä¾›ä¸´æ—¶æµ‹è¯•ä½¿ç”¨ã€‘ '

with app.app_context():
    global gpts_configurations  # ç§»åˆ°ä½œç”¨åŸŸçš„æœ€å¼€å§‹

    # è¾“å‡ºç‰ˆæœ¬ä¿¡æ¯
    logger.info(f"==========================================")
    logger.info(f"Version: {VERSION}")
    logger.info(f"Update Info: {UPDATE_INFO}")

    logger.info(f"LOG_LEVEL: {LOG_LEVEL}")
    logger.info(f"NEED_LOG_TO_FILE: {NEED_LOG_TO_FILE}")

    logger.info(f"BOT_MODE_ENABLED: {BOT_MODE_ENABLED}")

    if BOT_MODE_ENABLED:
        logger.info(f"enabled_markdown_image_output: {BOT_MODE_ENABLED_MARKDOWN_IMAGE_OUTPUT}")
        logger.info(f"enabled_plain_image_url_output: {BOT_MODE_ENABLED_PLAIN_IMAGE_URL_OUTPUT}")
        logger.info(f"enabled_bing_reference_output: {BOT_MODE_ENABLED_BING_REFERENCE_OUTPUT}")
        logger.info(f"enabled_plugin_output: {BOT_MODE_ENABLED_CODE_BLOCK_OUTPUT}")

    logger.info(f"REFRESH_TOACCESS_ENABLEOAI: {REFRESH_TOACCESS_ENABLEOAI}")

    if not REFRESH_TOACCESS_ENABLEOAI:
        logger.info(f"REFRESH_TOACCESS_OAIFREE_REFRESHTOACCESS_URL : {REFRESH_TOACCESS_OAIFREE_REFRESHTOACCESS_URL}")

    if BOT_MODE_ENABLED:
        logger.info(f"enabled_markdown_image_output: {BOT_MODE_ENABLED_MARKDOWN_IMAGE_OUTPUT}")
        logger.info(f"enabled_plain_image_url_output: {BOT_MODE_ENABLED_PLAIN_IMAGE_URL_OUTPUT}")
        logger.info(f"enabled_bing_reference_output: {BOT_MODE_ENABLED_BING_REFERENCE_OUTPUT}")
        logger.info(f"enabled_plugin_output: {BOT_MODE_ENABLED_CODE_BLOCK_OUTPUT}")

    # oaiFreeToV1Api_refresh

    logger.info(f"REFRESH_TOACCESS_ENABLEOAI: {REFRESH_TOACCESS_ENABLEOAI}")
    logger.info(f"REFRESH_TOACCESS_OAIFREE_REFRESHTOACCESS_URL : {REFRESH_TOACCESS_OAIFREE_REFRESHTOACCESS_URL}")
    logger.info(f"STEAM_SLEEP_TIME: {STEAM_SLEEP_TIME}")

    if not BASE_URL:
        raise Exception('upstream_base_url is not set')
    else:
        logger.info(f"upstream_base_url: {BASE_URL}")
    if not PROXY_API_PREFIX:
        logger.warning('upstream_api_prefix is not set')
    else:
        logger.info(f"upstream_api_prefix: {PROXY_API_PREFIX}")

    if USE_OAIUSERCONTENT_URL == False:
        # æ£€æµ‹./imageså’Œ./filesæ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º
        if not os.path.exists('./images'):
            os.makedirs('./images')
        if not os.path.exists('./files'):
            os.makedirs('./files')

    if not UPLOAD_BASE_URL:
        if USE_OAIUSERCONTENT_URL:
            logger.info("backend_container_url æœªè®¾ç½®ï¼Œå°†ä½¿ç”¨ oaiusercontent.com ä½œä¸ºå›¾ç‰‡åŸŸå")
        else:
            logger.warning("backend_container_url æœªè®¾ç½®ï¼Œå›¾ç‰‡ç”ŸæˆåŠŸèƒ½å°†æ— æ³•æ­£å¸¸ä½¿ç”¨")


    else:
        logger.info(f"backend_container_url: {UPLOAD_BASE_URL}")

    if not KEY_FOR_GPTS_INFO:
        logger.warning("key_for_gpts_info æœªè®¾ç½®ï¼Œè¯·å°† gpts.json ä¸­ä»…ä¿ç•™ â€œ{}â€ ä½œä¸ºå†…å®¹")
    else:
        logger.info(f"key_for_gpts_info: {KEY_FOR_GPTS_INFO}")

    if not API_PREFIX:
        logger.warning("backend_container_api_prefix æœªè®¾ç½®ï¼Œå®‰å…¨æ€§ä¼šæœ‰æ‰€ä¸‹é™")
        logger.info(f'Chat æ¥å£ URI: /v1/chat/completions')
        logger.info(f'ç»˜å›¾æ¥å£ URI: /v1/images/generations')
    else:
        logger.info(f"backend_container_api_prefix: {API_PREFIX}")
        logger.info(f'Chat æ¥å£ URI: /{API_PREFIX}/v1/chat/completions')
        logger.info(f'ç»˜å›¾æ¥å£ URI: /{API_PREFIX}/v1/images/generations')

    logger.info(f"need_delete_conversation_after_response: {NEED_DELETE_CONVERSATION_AFTER_RESPONSE}")

    logger.info(f"use_oaiusercontent_url: {USE_OAIUSERCONTENT_URL}")

    logger.info(f"use_pandora_file_server: False")

    logger.info(f"custom_arkose_url: {CUSTOM_ARKOSE}")

    if CUSTOM_ARKOSE:
        logger.info(f"arkose_urls: {ARKOSE_URLS}")

    logger.info(f"DALLE_prompt_prefix: {DALLE_PROMPT_PREFIX}")

    logger.info(f"==========================================")

    # æ›´æ–° gpts_configurations åˆ—è¡¨ï¼Œæ”¯æŒå¤šä¸ªæ˜ å°„
    gpts_configurations = []
    for name in GPT_4_S_New_Names:
        gpts_configurations.append({
            "name": name.strip(),
            "ori_name": "gpt-4-s"
        })
    for name in GPT_4_MOBILE_NEW_NAMES:
        gpts_configurations.append({
            "name": name.strip(),
            "ori_name": "gpt-4-mobile"
        })
    for name in GPT_3_5_NEW_NAMES:
        gpts_configurations.append({
            "name": name.strip(),
            "ori_name": "gpt-3.5-turbo"
        })
    for name in GPT_4_O_NEW_NAMES:
        gpts_configurations.append({
            "name": name.strip(),
            "ori_name": "gpt-4-o"
        })
    for name in GPT_4_O_MINI_NEW_NAMES:
        gpts_configurations.append({
            "name": name.strip(),
            "ori_name": "gpt-4o-mini"
        })
    for name in O1_PREVIEW_NEW_NAMES:
        gpts_configurations.append({
            "name": name.strip(),
            "ori_name": "o1-preview"
        })
    for name in O1_MINI_NEW_NAMES:
        gpts_configurations.append({
            "name": name.strip(),
            "ori_name": "o1-mini"
        })
    logger.info(f"GPTS é…ç½®ä¿¡æ¯")

    # åŠ è½½é…ç½®å¹¶æ·»åŠ åˆ°å…¨å±€åˆ—è¡¨
    gpts_data = load_gpts_config("./data/gpts.json")
    add_config_to_global_list(BASE_URL, getPROXY_API_PREFIX(lock), gpts_data)
    # print("å½“å‰å¯ç”¨GPTSï¼š" + get_accessible_model_list())
    # è¾“å‡ºå½“å‰å¯ç”¨ GPTS name
    # è·å–å½“å‰å¯ç”¨çš„ GPTS æ¨¡å‹åˆ—è¡¨
    accessible_model_list = get_accessible_model_list()
    logger.info(f"å½“å‰å¯ç”¨ GPTS åˆ—è¡¨: {accessible_model_list}")

    # æ£€æŸ¥åˆ—è¡¨ä¸­æ˜¯å¦æœ‰é‡å¤çš„æ¨¡å‹åç§°
    if len(accessible_model_list) != len(set(accessible_model_list)):
        raise Exception("æ£€æµ‹åˆ°é‡å¤çš„æ¨¡å‹åç§°ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶ã€‚")

    logger.info(f"==========================================")

    # print(f"GPTs Payload ç”Ÿæˆæµ‹è¯•")

    # print(f"gpt-4-classic: {generate_gpts_payload('gpt-4-classic', [])}")


# å®šä¹‰è·å– token çš„å‡½æ•°
def get_token():
    # ä»ç¯å¢ƒå˜é‡è·å– URL åˆ—è¡¨ï¼Œå¹¶å»é™¤æ¯ä¸ª URL å‘¨å›´çš„ç©ºç™½å­—ç¬¦
    api_urls = [url.strip() for url in ARKOSE_URLS.split(",")]

    for url in api_urls:
        if not url:
            continue

        full_url = f"{url}/api/arkose/token"
        payload = {'type': 'gpt-4'}

        try:
            response = requests.post(full_url, data=payload)
            if response.status_code == 200:
                token = response.json().get('token')
                # ç¡®ä¿ token å­—æ®µå­˜åœ¨ä¸”ä¸æ˜¯ None æˆ–ç©ºå­—ç¬¦ä¸²
                if token:
                    logger.debug(f"æˆåŠŸä» {url} è·å– arkose token")
                    return token
                else:
                    logger.error(f"è·å–çš„ token å“åº”æ— æ•ˆ: {token}")
            else:
                logger.error(f"è·å– arkose token å¤±è´¥: {response.status_code}, {response.text}")
        except requests.RequestException as e:
            logger.error(f"è¯·æ±‚å¼‚å¸¸: {e}")

    raise Exception("è·å– arkose token å¤±è´¥")


import os


def get_image_dimensions(file_content):
    with Image.open(BytesIO(file_content)) as img:
        return img.width, img.height


def determine_file_use_case(mime_type):
    multimodal_types = ["image/jpeg", "image/webp", "image/png", "image/gif"]
    my_files_types = ["text/x-php", "application/msword", "text/x-c", "text/html",
                      "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                      "application/json", "text/javascript", "application/pdf",
                      "text/x-java", "text/x-tex", "text/x-typescript", "text/x-sh",
                      "text/x-csharp", "application/vnd.openxmlformats-officedocument.presentationml.presentation",
                      "text/x-c++", "application/x-latext", "text/markdown", "text/plain",
                      "text/x-ruby", "text/x-script.python"]

    if mime_type in multimodal_types:
        return "multimodal"
    elif mime_type in my_files_types:
        return "my_files"
    else:
        return "ace_upload"


def upload_file(file_content, mime_type, api_key, proxy_api_prefix):
    logger.debug("æ–‡ä»¶ä¸Šä¼ å¼€å§‹")

    width = None
    height = None
    if mime_type.startswith('image/'):
        try:
            width, height = get_image_dimensions(file_content)
        except Exception as e:
            logger.error(f"å›¾ç‰‡ä¿¡æ¯è·å–å¼‚å¸¸, åˆ‡æ¢ä¸ºtext/plainï¼š {e}")
            mime_type = 'text/plain'

    # logger.debug(f"æ–‡ä»¶å†…å®¹: {file_content}")
    file_size = len(file_content)
    logger.debug(f"æ–‡ä»¶å¤§å°: {file_size}")
    file_extension = get_file_extension(mime_type)
    logger.debug(f"æ–‡ä»¶æ‰©å±•å: {file_extension}")
    sha256_hash = hashlib.sha256(file_content).hexdigest()
    logger.debug(f"sha256_hash: {sha256_hash}")
    file_name = f"{sha256_hash}{file_extension}"
    logger.debug(f"æ–‡ä»¶å: {file_name}")

    logger.debug(f"Use Case: {determine_file_use_case(mime_type)}")

    if determine_file_use_case(mime_type) == "ace_upload":
        mime_type = ''
        logger.debug(f"éå·²çŸ¥æ–‡ä»¶ç±»å‹ï¼ŒMINEç½®ç©º")

    # ç¬¬1æ­¥ï¼šè°ƒç”¨/backend-api/filesæ¥å£è·å–ä¸Šä¼ URL
    upload_api_url = f"{BASE_URL}{proxy_api_prefix}/backend-api/files"
    upload_request_payload = {
        "file_name": file_name,
        "file_size": file_size,
        "use_case": determine_file_use_case(mime_type)
    }
    headers = {
        "Authorization": f"Bearer {api_key}"
    }
    upload_response = requests.post(upload_api_url, json=upload_request_payload, headers=headers)
    logger.debug(f"upload_response: {upload_response.text}")
    if upload_response.status_code != 200:
        raise Exception("Failed to get upload URL")

    upload_data = upload_response.json()
    upload_url = upload_data.get("upload_url")
    logger.debug(f"upload_url: {upload_url}")
    file_id = upload_data.get("file_id")
    logger.debug(f"file_id: {file_id}")

    # ç¬¬2æ­¥ï¼šä¸Šä¼ æ–‡ä»¶
    put_headers = {
        'Content-Type': mime_type,
        'x-ms-blob-type': 'BlockBlob'  # æ·»åŠ è¿™ä¸ªå¤´éƒ¨
    }
    put_response = requests.put(upload_url, data=file_content, headers=put_headers)
    if put_response.status_code != 201:
        logger.debug(f"put_response: {put_response.text}")
        logger.debug(f"put_response status_code: {put_response.status_code}")
        raise Exception("Failed to upload file")

    # ç¬¬3æ­¥ï¼šæ£€æµ‹ä¸Šä¼ æ˜¯å¦æˆåŠŸå¹¶æ£€æŸ¥å“åº”
    check_url = f"{BASE_URL}{proxy_api_prefix}/backend-api/files/{file_id}/uploaded"
    check_response = requests.post(check_url, json={}, headers=headers)
    logger.debug(f"check_response: {check_response.text}")
    if check_response.status_code != 200:
        raise Exception("Failed to check file upload completion")

    check_data = check_response.json()
    if check_data.get("status") != "success":
        raise Exception("File upload completion check not successful")

    return {
        "file_id": file_id,
        "file_name": file_name,
        "size_bytes": file_size,
        "mimeType": mime_type,
        "width": width,
        "height": height
    }


def get_file_metadata(file_content, mime_type, api_key, proxy_api_prefix):
    sha256_hash = hashlib.sha256(file_content).hexdigest()
    logger.debug(f"sha256_hash: {sha256_hash}")
    # é¦–å…ˆå°è¯•ä»Redisä¸­è·å–æ•°æ®
    cached_data = redis_client.get(sha256_hash)
    if cached_data is not None:
        # å¦‚æœåœ¨Redisä¸­æ‰¾åˆ°äº†æ•°æ®ï¼Œè§£ç åç›´æ¥è¿”å›
        logger.info(f"ä»Redisä¸­è·å–åˆ°æ–‡ä»¶ç¼“å­˜æ•°æ®")
        cache_file_data = json.loads(cached_data.decode())

        tag = True
        file_id = cache_file_data.get("file_id")
        # æ£€æµ‹ä¹‹å‰çš„æ–‡ä»¶æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
        check_url = f"{BASE_URL}{proxy_api_prefix}/backend-api/files/{file_id}/uploaded"
        headers = {
            "Authorization": f"Bearer {api_key}"
        }
        check_response = requests.post(check_url, json={}, headers=headers)
        logger.debug(f"check_response: {check_response.text}")
        if check_response.status_code != 200:
            tag = False

        check_data = check_response.json()
        if check_data.get("status") != "success":
            tag = False
        if tag:
            logger.info(f"Redisä¸­çš„æ–‡ä»¶ç¼“å­˜æ•°æ®æœ‰æ•ˆï¼Œå°†ä½¿ç”¨ç¼“å­˜æ•°æ®")
            return cache_file_data
        else:
            logger.info(f"Redisä¸­çš„æ–‡ä»¶ç¼“å­˜æ•°æ®å·²å¤±æ•ˆï¼Œé‡æ–°ä¸Šä¼ æ–‡ä»¶")

    else:
        logger.info(f"Redisä¸­æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶ç¼“å­˜æ•°æ®")
    # å¦‚æœRedisä¸­æ²¡æœ‰ï¼Œä¸Šä¼ æ–‡ä»¶å¹¶ä¿å­˜æ–°æ•°æ®
    new_file_data = upload_file(file_content, mime_type, api_key, proxy_api_prefix)
    mime_type = new_file_data.get('mimeType')
    # ä¸ºå›¾ç‰‡ç±»å‹æ–‡ä»¶æ·»åŠ å®½åº¦å’Œé«˜åº¦ä¿¡æ¯
    if mime_type.startswith('image/'):
        width, height = get_image_dimensions(file_content)
        new_file_data['width'] = width
        new_file_data['height'] = height

    # å°†æ–°çš„æ–‡ä»¶æ•°æ®å­˜å…¥Redis
    redis_client.set(sha256_hash, json.dumps(new_file_data))

    return new_file_data


def get_file_extension(mime_type):
    # åŸºäº MIME ç±»å‹è¿”å›æ–‡ä»¶æ‰©å±•åçš„æ˜ å°„è¡¨
    extension_mapping = {
        "image/jpeg": ".jpg",
        "image/png": ".png",
        "image/gif": ".gif",
        "image/webp": ".webp",
        "text/x-php": ".php",
        "application/msword": ".doc",
        "text/x-c": ".c",
        "text/html": ".html",
        "application/vnd.openxmlformats-officedocument.wordprocessingml.document": ".docx",
        "application/json": ".json",
        "text/javascript": ".js",
        "application/pdf": ".pdf",
        "text/x-java": ".java",
        "text/x-tex": ".tex",
        "text/x-typescript": ".ts",
        "text/x-sh": ".sh",
        "text/x-csharp": ".cs",
        "application/vnd.openxmlformats-officedocument.presentationml.presentation": ".pptx",
        "text/x-c++": ".cpp",
        "application/x-latext": ".latex",  # è¿™é‡Œå¯èƒ½éœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
        "text/markdown": ".md",
        "text/plain": ".txt",
        "text/x-ruby": ".rb",
        "text/x-script.python": ".py",
        # å…¶ä»– MIME ç±»å‹å’Œæ‰©å±•å...
    }
    return extension_mapping.get(mime_type, mimetypes.guess_extension(mime_type))


my_files_types = [
    "text/x-php", "application/msword", "text/x-c", "text/html",
    "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
    "application/json", "text/javascript", "application/pdf",
    "text/x-java", "text/x-tex", "text/x-typescript", "text/x-sh",
    "text/x-csharp", "application/vnd.openxmlformats-officedocument.presentationml.presentation",
    "text/x-c++", "application/x-latext", "text/markdown", "text/plain",
    "text/x-ruby", "text/x-script.python"
]


# å®šä¹‰å‘é€è¯·æ±‚çš„å‡½æ•°
def send_text_prompt_and_get_response(messages, api_key, account_id, stream, model, proxy_api_prefix):
    url = f"{BASE_URL}{proxy_api_prefix}/backend-api/conversation"
    headers = {
        "Authorization": f"Bearer {api_key}"
    }
    # æŸ¥æ‰¾æ¨¡å‹é…ç½®
    model_config = find_model_config(model)
    ori_model_name = ''
    if model_config:
        # æ£€æŸ¥æ˜¯å¦æœ‰ ori_name
        ori_model_name = model_config.get('ori_name', model)

    formatted_messages = []
    # logger.debug(f"åŸå§‹ messages: {messages}")
    for message in messages:
        message_id = str(uuid.uuid4())
        content = message.get("content")

        if isinstance(content, list) and ori_model_name not in ['gpt-3.5-turbo']:
            logger.debug(f"gpt-vision è°ƒç”¨")
            new_parts = []
            attachments = []
            contains_image = False  # æ ‡è®°æ˜¯å¦åŒ…å«å›¾ç‰‡

            for part in content:
                if isinstance(part, dict) and "type" in part:
                    if part["type"] == "text":
                        new_parts.append(part["text"])
                    elif part["type"] == "image_url":
                        # logger.debug(f"image_url: {part['image_url']}")
                        file_url = part["image_url"]["url"]
                        if file_url.startswith('data:'):
                            # å¤„ç† base64 ç¼–ç çš„æ–‡ä»¶æ•°æ®
                            mime_type, base64_data = file_url.split(';')[0], file_url.split(',')[1]
                            mime_type = mime_type.split(':')[1]
                            try:
                                file_content = base64.b64decode(base64_data)
                            except Exception as e:
                                logger.error(f"ç±»å‹ä¸º {mime_type} çš„ base64 ç¼–ç æ•°æ®è§£ç å¤±è´¥: {e}")
                                continue
                        else:
                            # å¤„ç†æ™®é€šçš„æ–‡ä»¶URL
                            try:
                                tmp_user_agent = ua.random
                                logger.debug(f"éšæœº User-Agent: {tmp_user_agent}")
                                tmp_headers = {
                                    'User-Agent': tmp_user_agent
                                }
                                file_response = requests.get(url=file_url, headers=tmp_headers)
                                file_content = file_response.content
                                mime_type = file_response.headers.get('Content-Type', '').split(';')[0].strip()
                            except Exception as e:
                                logger.error(f"è·å–æ–‡ä»¶ {file_url} å¤±è´¥: {e}")
                                continue

                        logger.debug(f"mime_type: {mime_type}")
                        file_metadata = get_file_metadata(file_content, mime_type, api_key, proxy_api_prefix)

                        mime_type = file_metadata["mimeType"]
                        logger.debug(f"å¤„ç†å mime_type: {mime_type}")

                        if mime_type.startswith('image/'):
                            contains_image = True
                            new_part = {
                                "asset_pointer": f"file-service://{file_metadata['file_id']}",
                                "size_bytes": file_metadata["size_bytes"],
                                "width": file_metadata["width"],
                                "height": file_metadata["height"]
                            }
                            new_parts.append(new_part)

                        attachment = {
                            "name": file_metadata["file_name"],
                            "id": file_metadata["file_id"],
                            "mimeType": file_metadata["mimeType"],
                            "size": file_metadata["size_bytes"]  # æ·»åŠ æ–‡ä»¶å¤§å°
                        }

                        if mime_type.startswith('image/'):
                            attachment.update({
                                "width": file_metadata["width"],
                                "height": file_metadata["height"]
                            })
                        elif mime_type in my_files_types:
                            attachment.update({"fileTokenSize": len(file_metadata["file_name"])})

                        attachments.append(attachment)
                else:
                    # ç¡®ä¿ part æ˜¯å­—ç¬¦ä¸²
                    text_part = str(part) if not isinstance(part, str) else part
                    new_parts.append(text_part)

            content_type = "multimodal_text" if contains_image else "text"
            formatted_message = {
                "id": message_id,
                "author": {"role": message.get("role")},
                "content": {"content_type": content_type, "parts": new_parts},
                "metadata": {"attachments": attachments}
            }
            formatted_messages.append(formatted_message)
            logger.critical(f"formatted_message: {formatted_message}")

        else:
            # å¤„ç†å•ä¸ªæ–‡æœ¬æ¶ˆæ¯çš„æƒ…å†µ
            formatted_message = {
                "id": message_id,
                "author": {"role": message.get("role")},
                "content": {"content_type": "text", "parts": [content]},
                "metadata": {}
            }
            formatted_messages.append(formatted_message)

    # logger.debug(f"formatted_messages: {formatted_messages}")
    # return
    payload = {}

    logger.info(f"model: {model}")

    # æŸ¥æ‰¾æ¨¡å‹é…ç½®
    model_config = find_model_config(model)
    if model_config or 'gpt-4-gizmo-' in model:
        # æ£€æŸ¥æ˜¯å¦æœ‰ ori_name
        if model_config:
            ori_model_name = model_config.get('ori_name', model)
            logger.info(f"åŸæ¨¡å‹å: {ori_model_name}")
        else:
            logger.info(f"è¯·æ±‚æ¨¡å‹å: {model}")
            ori_model_name = model
        if ori_model_name == 'gpt-4-s':
            payload = {
                # æ„å»º payload
                "action": "next",
                "messages": formatted_messages,
                "parent_message_id": str(uuid.uuid4()),
                "model": "gpt-4",
                "timezone_offset_min": -480,
                "suggestions": [],
                "history_and_training_disabled": False,
                "conversation_mode": {"kind": "primary_assistant"}, "force_paragen": False, "force_rate_limit": False
            }
        elif ori_model_name == 'gpt-4-mobile':
            payload = {
                # æ„å»º payload
                "action": "next",
                "messages": formatted_messages,
                "parent_message_id": str(uuid.uuid4()),
                "model": "gpt-4",
                "timezone_offset_min": -480,
                "suggestions": [],
                "history_and_training_disabled": False,
                "conversation_mode": {"kind": "primary_assistant"}, "force_paragen": False, "force_rate_limit": False
            }
        elif ori_model_name == 'gpt-3.5-turbo':
            payload = {
                # æ„å»º payload
                "action": "next",
                "messages": formatted_messages,
                "parent_message_id": str(uuid.uuid4()),
                "model": "gpt-4o-mini",
                "timezone_offset_min": -480,
                "suggestions": [
                    "What are 5 creative things I could do with my kids' art? I don't want to throw them away, "
                    "but it's also so much clutter.",
                    "I want to cheer up my friend who's having a rough day. Can you suggest a couple short and sweet "
                    "text messages to go with a kitten gif?",
                    "Come up with 5 concepts for a retro-style arcade game.",
                    "I have a photoshoot tomorrow. Can you recommend me some colors and outfit options that will look "
                    "good on camera?"
                ],
                "history_and_training_disabled": False,
                "arkose_token": None,
                "conversation_mode": {
                    "kind": "primary_assistant"
                },
                "force_paragen": False,
                "force_paragen_model_slug": "",
                "force_rate_limit": False
            }
        elif ori_model_name == 'gpt-4-o':
            payload = {
                # æ„å»º payload
                "action": "next",
                "messages": formatted_messages,
                "parent_message_id": str(uuid.uuid4()),
                "model": "gpt-4o",
                "timezone_offset_min": -480,
                "suggestions": [
                    "What are 5 creative things I could do with my kids' art? I don't want to throw them away, but it's also so much clutter.",
                    "I want to cheer up my friend who's having a rough day. Can you suggest a couple short and sweet text messages to go with a kitten gif?",
                    "Come up with 5 concepts for a retro-style arcade game.",
                    "I have a photoshoot tomorrow. Can you recommend me some colors and outfit options that will look good on camera?"
                ],
                "history_and_training_disabled": False,
                "arkose_token": None,
                "conversation_mode": {
                    "kind": "primary_assistant"
                },
                "force_paragen": False,
                "force_rate_limit": False
            }
        elif ori_model_name == 'gpt-4o-mini':
            payload = {
                # æ„å»º payload
                "action": "next",
                "messages": formatted_messages,
                "parent_message_id": str(uuid.uuid4()),
                "model": "gpt-4o-mini",
                "timezone_offset_min": -480,
                "suggestions": [
                    "What are 5 creative things I could do with my kids' art? I don't want to throw them away, "
                    "but it's also so much clutter.",
                    "I want to cheer up my friend who's having a rough day. Can you suggest a couple short and sweet "
                    "text messages to go with a kitten gif?",
                    "Come up with 5 concepts for a retro-style arcade game.",
                    "I have a photoshoot tomorrow. Can you recommend me some colors and outfit options that will look "
                    "good on camera?"
                ],
                "history_and_training_disabled": False,
                "arkose_token": None,
                "conversation_mode": {
                    "kind": "primary_assistant"
                },
                "force_paragen": False,
                "force_paragen_model_slug": "",
                "force_rate_limit": False
            }
        elif ori_model_name == 'o1-preview':
            payload = {
                "action": "next",
                "messages": formatted_messages,
                "parent_message_id": str(uuid.uuid4()),
                "model": "o1-preview",
                "timezone_offset_min": -480,
                "suggestions": [
                    "What are 5 creative things I could do with my kids' art? I don't want to throw them away, "
                    "but it's also so much clutter.",
                    "I want to cheer up my friend who's having a rough day. Can you suggest a couple short and sweet "
                    "text messages to go with a kitten gif?",
                    "Come up with 5 concepts for a retro-style arcade game.",
                    "I have a photoshoot tomorrow. Can you recommend me some colors and outfit options that will look "
                    "good on camera?"
                ],
                "variant_purpose": "comparison_implicit",
                "history_and_training_disabled": False,
                "conversation_mode": {
                    "kind": "primary_assistant"
                },
                "force_paragen": False,
                "force_paragen_model_slug": "",
                "force_nulligen": False,
                "force_rate_limit": False,
                "reset_rate_limits": False,
                "force_use_sse": True,
            }
        elif ori_model_name == 'o1-mini':
            payload = {
                "action": "next",
                "messages": formatted_messages,
                "parent_message_id": str(uuid.uuid4()),
                "model": "o1-mini",
                "timezone_offset_min": -480,
                "suggestions": [
                    "What are 5 creative things I could do with my kids' art? I don't want to throw them away, "
                    "but it's also so much clutter.",
                    "I want to cheer up my friend who's having a rough day. Can you suggest a couple short and sweet "
                    "text messages to go with a kitten gif?",
                    "Come up with 5 concepts for a retro-style arcade game.",
                    "I have a photoshoot tomorrow. Can you recommend me some colors and outfit options that will look "
                    "good on camera?"
                ],
                "variant_purpose": "comparison_implicit",
                "history_and_training_disabled": False,
                "conversation_mode": {
                    "kind": "primary_assistant"
                },
                "force_paragen": False,
                "force_paragen_model_slug": "",
                "force_nulligen": False,
                "force_rate_limit": False,
                "reset_rate_limits": False,
                "force_use_sse": True,
            }

        elif 'gpt-4-gizmo-' in model:
            payload = generate_gpts_payload(model, formatted_messages)
            if not payload:
                global gpts_configurations
                # å‡è®¾ modelæ˜¯ 'gpt-4-gizmo-123'
                split_name = model.split('gpt-4-gizmo-')
                model_id = split_name[1] if len(split_name) > 1 else None
                gizmo_info = fetch_gizmo_info(BASE_URL, proxy_api_prefix, model_id)
                logging.info(gizmo_info)

                # å¦‚æœæˆåŠŸè·å–åˆ°æ•°æ®ï¼Œåˆ™å°†å…¶å­˜å…¥ Redis
                if gizmo_info:
                    redis_client.set(model_id, str(gizmo_info))
                    logger.info(f"Cached gizmo info for {model}, {model_id}")
                    # æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦å·²ç»åœ¨åˆ—è¡¨ä¸­
                    if not any(d['name'] == model for d in gpts_configurations):
                        gpts_configurations.append({
                            'name': model,
                            'id': model_id,
                            'config': gizmo_info
                        })
                    else:
                        logger.info(f"Model already exists in the list, skipping...")
                    payload = generate_gpts_payload(model, formatted_messages)
                else:
                    raise Exception('KEY_FOR_GPTS_INFO is not accessible')
        else:
            payload = generate_gpts_payload(model, formatted_messages)
            if not payload:
                raise Exception('model is not accessible')
        # æ ¹æ®NEED_DELETE_CONVERSATION_AFTER_RESPONSEä¿®æ”¹history_and_training_disabled
        if NEED_DELETE_CONVERSATION_AFTER_RESPONSE:
            logger.debug(f"æ˜¯å¦ä¿ç•™ä¼šè¯: {NEED_DELETE_CONVERSATION_AFTER_RESPONSE == False}")
            payload['history_and_training_disabled'] = True
        if ori_model_name not in ['gpt-3.5-turbo']:
            if CUSTOM_ARKOSE:
                token = get_token()
                payload["arkose_token"] = token
                # åœ¨headersä¸­æ·»åŠ æ–°å­—æ®µ
                headers["Openai-Sentinel-Arkose-Token"] = token

            # ç”¨äºè°ƒç”¨ChatGPT Teamæ¬¡æ•°
            if account_id:
                headers["ChatGPT-Account-ID"] = account_id

        logger.debug(f"headers: {headers}")
        logger.debug(f"payload: {payload}")
        response = requests.post(url, headers=headers, json=payload, stream=True)
        # print(response)
        return response


def delete_conversation(conversation_id, api_key, proxy_api_prefix):
    logger.info(f"å‡†å¤‡åˆ é™¤çš„ä¼šè¯idï¼š {conversation_id}")
    if not NEED_DELETE_CONVERSATION_AFTER_RESPONSE:
        logger.info(f"è‡ªåŠ¨åˆ é™¤ä¼šè¯åŠŸèƒ½å·²ç¦ç”¨")
        return
    if conversation_id and NEED_DELETE_CONVERSATION_AFTER_RESPONSE:
        patch_url = f"{BASE_URL}{proxy_api_prefix}/backend-api/conversation/{conversation_id}"
        patch_headers = {
            "Authorization": f"Bearer {api_key}",
        }
        patch_data = {"is_visible": False}
        response = requests.patch(patch_url, headers=patch_headers, json=patch_data)

        if response.status_code == 200:
            logger.info(f"åˆ é™¤ä¼šè¯ {conversation_id} æˆåŠŸ")
        else:
            logger.error(f"PATCH è¯·æ±‚å¤±è´¥: {response.text}")


from PIL import Image
import io


def save_image(image_data, path='images'):
    try:
        # print(f"image_data: {image_data}")
        if not os.path.exists(path):
            os.makedirs(path)
        current_time = datetime.now().strftime('%Y%m%d%H%M%S')
        filename = f'image_{current_time}.png'
        full_path = os.path.join(path, filename)
        logger.debug(f"å®Œæ•´çš„æ–‡ä»¶è·¯å¾„: {full_path}")  # æ‰“å°å®Œæ•´è·¯å¾„
        # print(f"filename: {filename}")
        # ä½¿ç”¨ PIL æ‰“å¼€å›¾åƒæ•°æ®
        with Image.open(io.BytesIO(image_data)) as image:
            # ä¿å­˜ä¸º PNG æ ¼å¼
            image.save(os.path.join(path, filename), 'PNG')

        logger.debug(f"ä¿å­˜å›¾ç‰‡æˆåŠŸ: {filename}")

        return os.path.join(path, filename)
    except Exception as e:
        logger.error(f"ä¿å­˜å›¾ç‰‡æ—¶å‡ºç°å¼‚å¸¸: {e}")


def unicode_to_chinese(unicode_string):
    # é¦–å…ˆå°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ ‡å‡†çš„ JSON æ ¼å¼å­—ç¬¦ä¸²
    json_formatted_str = json.dumps(unicode_string)
    # ç„¶åå°† JSON æ ¼å¼çš„å­—ç¬¦ä¸²è§£æå›æ­£å¸¸çš„å­—ç¬¦ä¸²
    return json.loads(json_formatted_str)


import re


# è¾…åŠ©å‡½æ•°ï¼šæ£€æŸ¥æ˜¯å¦ä¸ºåˆæ³•çš„å¼•ç”¨æ ¼å¼æˆ–æ­£åœ¨æ„å»ºä¸­çš„å¼•ç”¨æ ¼å¼
def is_valid_citation_format(text):
    # å®Œæ•´ä¸”åˆæ³•çš„å¼•ç”¨æ ¼å¼ï¼Œå…è®¸ç´§è·Ÿå¦ä¸€ä¸ªèµ·å§‹å¼•ç”¨æ ‡è®°
    if re.fullmatch(r'\u3010\d+\u2020(source|\u6765\u6e90)\u3011\u3010?', text):
        return True

    # å®Œæ•´ä¸”åˆæ³•çš„å¼•ç”¨æ ¼å¼

    if re.fullmatch(r'\u3010\d+\u2020(source|\u6765\u6e90)\u3011', text):
        return True

    # åˆæ³•çš„éƒ¨åˆ†æ„å»ºæ ¼å¼
    if re.fullmatch(r'\u3010(\d+)?(\u2020(source|\u6765\u6e90)?)?', text):
        return True

    # ä¸åˆæ³•çš„æ ¼å¼
    return False


# è¾…åŠ©å‡½æ•°ï¼šæ£€æŸ¥æ˜¯å¦ä¸ºå®Œæ•´çš„å¼•ç”¨æ ¼å¼
# æ£€æŸ¥æ˜¯å¦ä¸ºå®Œæ•´çš„å¼•ç”¨æ ¼å¼
def is_complete_citation_format(text):
    return bool(re.fullmatch(r'\u3010\d+\u2020(source|\u6765\u6e90)\u3011\u3010?', text))


# æ›¿æ¢å®Œæ•´çš„å¼•ç”¨æ ¼å¼
def replace_complete_citation(text, citations):
    def replace_match(match):
        citation_number = match.group(1)
        for citation in citations:
            cited_message_idx = citation.get('metadata', {}).get('extra', {}).get('cited_message_idx')
            logger.debug(f"cited_message_idx: {cited_message_idx}")
            logger.debug(f"citation_number: {citation_number}")
            logger.debug(f"is citation_number == cited_message_idx: {cited_message_idx == int(citation_number)}")
            logger.debug(f"citation: {citation}")
            if cited_message_idx == int(citation_number):
                url = citation.get("metadata", {}).get("url", "")
                if ((BOT_MODE_ENABLED == False) or (
                        BOT_MODE_ENABLED == True and BOT_MODE_ENABLED_BING_REFERENCE_OUTPUT == True)):
                    return f"[[{citation_number}]({url})]"
                else:
                    return ""
        # return match.group(0)  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„å¼•ç”¨ï¼Œè¿”å›åŸæ–‡æœ¬
        logger.critical(f"æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„å¼•ç”¨ï¼Œèˆå¼ƒ{match.group(0)}å¼•ç”¨")
        return ""

    # ä½¿ç”¨ finditer æ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ¹é…é¡¹
    match_iter = re.finditer(r'\u3010(\d+)\u2020(source|\u6765\u6e90)\u3011', text)
    first_match = next(match_iter, None)

    if first_match:
        start, end = first_match.span()
        replaced_text = text[:start] + replace_match(first_match) + text[end:]
        remaining_text = text[end:]
    else:
        replaced_text = text
        remaining_text = ""

    is_potential_citation = is_valid_citation_format(remaining_text)

    # æ›¿æ¢æ‰replaced_textæœ«å°¾çš„remaining_text

    logger.debug(f"replaced_text: {replaced_text}")
    logger.debug(f"remaining_text: {remaining_text}")
    logger.debug(f"is_potential_citation: {is_potential_citation}")
    if is_potential_citation:
        replaced_text = replaced_text[:-len(remaining_text)]

    return replaced_text, remaining_text, is_potential_citation


def is_valid_sandbox_combined_corrected_final_v2(text):
    # æ›´æ–°æ­£åˆ™è¡¨è¾¾å¼ä»¥åŒ…å«æ‰€æœ‰åˆæ³•æ ¼å¼
    patterns = [
        r'.*\(sandbox:\/[^)]*\)?',  # sandbox åè·Ÿè·¯å¾„ï¼ŒåŒ…æ‹¬ä¸å®Œæ•´è·¯å¾„
        r'.*\(',  # åªæœ‰ "(" ä¹Ÿè§†ä¸ºåˆæ³•æ ¼å¼
        r'.*\(sandbox(:|$)',  # åŒ¹é… "(sandbox" æˆ– "(sandbox:"ï¼Œç¡®ä¿åé¢ä¸è·Ÿå…¶ä»–å­—ç¬¦æˆ–å­—ç¬¦ä¸²ç»“æŸ
        r'.*\(sandbox:.*\n*',  # åŒ¹é… "(sandbox:" åè·Ÿä»»æ„æ•°é‡çš„æ¢è¡Œç¬¦
    ]

    # æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ç¬¦åˆä»»ä¸€åˆæ³•æ ¼å¼
    return any(bool(re.fullmatch(pattern, text)) for pattern in patterns)


def is_complete_sandbox_format(text):
    # å®Œæ•´æ ¼å¼åº”è¯¥ç±»ä¼¼äº (sandbox:/xx/xx/xx æˆ– (sandbox:/xx/xx)
    pattern = r'.*\(sandbox\:\/[^)]+\)\n*'  # åŒ¹é… "(sandbox:" åè·Ÿä»»æ„æ•°é‡çš„æ¢è¡Œç¬¦
    return bool(re.fullmatch(pattern, text))


import urllib.parse
from urllib.parse import unquote


def replace_sandbox(text, conversation_id, message_id, api_key, proxy_api_prefix):
    def replace_match(match):
        sandbox_path = match.group(1)
        download_url = get_download_url(conversation_id, message_id, sandbox_path)
        if download_url == None:
            return "\n```\nError: æ²™ç®±æ–‡ä»¶ä¸‹è½½å¤±è´¥ï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºæ‚¨å¯ç”¨äº†éšç§æ¨¡å¼\n```"
        file_name = extract_filename(download_url)
        timestamped_file_name = timestamp_filename(file_name)
        if USE_OAIUSERCONTENT_URL == False:
            download_file(download_url, timestamped_file_name)
            return f"({UPLOAD_BASE_URL}/files/{timestamped_file_name})"
        else:
            return f"({download_url})"

    def get_download_url(conversation_id, message_id, sandbox_path):
        # æ¨¡æ‹Ÿå‘èµ·è¯·æ±‚ä»¥è·å–ä¸‹è½½ URL
        sandbox_info_url = f"{BASE_URL}{proxy_api_prefix}/backend-api/conversation/{conversation_id}/interpreter/download?message_id={message_id}&sandbox_path={sandbox_path}"

        headers = {
            "Authorization": f"Bearer {api_key}"
        }

        response = requests.get(sandbox_info_url, headers=headers)

        if response.status_code == 200:
            logger.debug(f"è·å–ä¸‹è½½ URL æˆåŠŸ: {response.json()}")
            return response.json().get("download_url")
        else:
            logger.error(f"è·å–ä¸‹è½½ URL å¤±è´¥: {response.text}")
            return None

    def extract_filename(url):
        # ä» URL ä¸­æå– filename å‚æ•°
        parsed_url = urllib.parse.urlparse(url)
        query_params = urllib.parse.parse_qs(parsed_url.query)
        filename = query_params.get("rscd", [""])[0].split("filename=")[-1]
        return filename

    def timestamp_filename(filename):
        # åœ¨æ–‡ä»¶åå‰åŠ ä¸Šå½“å‰æ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")

        # è§£ç URLç¼–ç çš„filename
        decoded_filename = unquote(filename)

        return f"{timestamp}_{decoded_filename}"

    def download_file(download_url, filename):
        # ä¸‹è½½å¹¶ä¿å­˜æ–‡ä»¶
        # ç¡®ä¿ ./files ç›®å½•å­˜åœ¨
        if not os.path.exists("./files"):
            os.makedirs("./files")
        file_path = f"./files/{filename}"
        with requests.get(download_url, stream=True) as r:
            with open(file_path, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)

    # æ›¿æ¢ (sandbox:xxx) æ ¼å¼çš„æ–‡æœ¬
    replaced_text = re.sub(r'\(sandbox:([^)]+)\)', replace_match, text)
    return replaced_text


def data_fetcher(upstream_response, data_queue, stop_event, last_data_time, api_key, chat_message_id, model,
                 proxy_api_prefix):
    all_new_text = ""

    first_output = True

    # å½“å‰æ—¶é—´æˆ³
    timestamp = int(time.time())

    buffer = ""
    last_full_text = ""  # ç”¨äºå­˜å‚¨ä¹‹å‰æ‰€æœ‰å‡ºç°è¿‡çš„ parts ç»„æˆçš„å®Œæ•´æ–‡æœ¬
    last_full_code = ""
    last_full_code_result = ""
    last_content_type = None  # ç”¨äºè®°å½•ä¸Šä¸€ä¸ªæ¶ˆæ¯çš„å†…å®¹ç±»å‹
    conversation_id = ''
    citation_buffer = ""
    citation_accumulating = False
    file_output_buffer = ""
    file_output_accumulating = False
    execution_output_image_url_buffer = ""
    execution_output_image_id_buffer = ""
    message = None
    try:
        for chunk in upstream_response.iter_content(chunk_size=1024):
            if stop_event.is_set():
                logger.info(f"æ¥å—åˆ°åœæ­¢ä¿¡å·ï¼Œåœæ­¢æ•°æ®å¤„ç†çº¿ç¨‹")
                break
            if chunk:
                buffer += chunk.decode('utf-8')
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ "event: ping"ï¼Œå¦‚æœå­˜åœ¨ï¼Œåˆ™åªä¿ç•™ "data:" åé¢çš„å†…å®¹
                if "event: ping" in buffer:
                    if "data:" in buffer:
                        buffer = buffer.split("data:", 1)[1]
                        buffer = "data:" + buffer
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç§»é™¤ç‰¹å®šæ ¼å¼çš„å­—ç¬¦ä¸²
                # print("åº”ç”¨æ­£åˆ™è¡¨è¾¾å¼ä¹‹å‰çš„ buffer:", buffer.replace('\n', '\\n'))
                buffer = re.sub(r'data: \d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d{6}(\r\n|\r|\n){2}', '', buffer)
                # print("åº”ç”¨æ­£åˆ™è¡¨è¾¾å¼ä¹‹åçš„ buffer:", buffer.replace('\n', '\\n'))

                while 'data:' in buffer and '\n\n' in buffer:
                    end_index = buffer.index('\n\n') + 2
                    complete_data, buffer = buffer[:end_index], buffer[end_index:]
                    try:
                        data_content = complete_data.replace('data: ', '').strip()
                        if not data_content:
                            continue
                        data_json = json.loads(data_content)
                        # print(f"data_json: {data_json}")
                        message = data_json.get("message", {})

                        if message == {} or message == None:
                            logger.debug(f"message ä¸ºç©º: data_json: {data_json}")

                        message_id = message.get("id")
                        message_status = message.get("status")
                        content = message.get("content", {})
                        role = message.get("author", {}).get("role")
                        content_type = content.get("content_type")

                        metadata = {}
                        citations = []
                        try:
                            metadata = message.get("metadata", {})
                            citations = metadata.get("citations", [])
                        except:
                            pass
                        name = message.get("author", {}).get("name")
                        if (
                                role == "user" or message_status == "finished_successfully" or role == "system") and role != "tool":
                            # å¦‚æœæ˜¯ç”¨æˆ·å‘æ¥çš„æ¶ˆæ¯ï¼Œç›´æ¥èˆå¼ƒ
                            continue
                        try:
                            conversation_id = data_json.get("conversation_id")
                            # print(f"conversation_id: {conversation_id}")
                            if conversation_id:
                                data_queue.put(('conversation_id', conversation_id))
                        except:
                            pass
                            # åªè·å–æ–°çš„éƒ¨åˆ†
                        new_text = ""
                        is_img_message = False
                        parts = content.get("parts", [])
                        for part in parts:
                            try:
                                # print(f"part: {part}")
                                # print(f"part type: {part.get('content_type')}")
                                if part.get('content_type') == 'image_asset_pointer':
                                    logger.debug(f"find img message~")
                                    is_img_message = True
                                    asset_pointer = part.get('asset_pointer').replace('file-service://', '')
                                    logger.debug(f"asset_pointer: {asset_pointer}")
                                    image_url = f"{BASE_URL}{proxy_api_prefix}/backend-api/files/{asset_pointer}/download"

                                    headers = {
                                        "Authorization": f"Bearer {api_key}"
                                    }
                                    image_response = requests.get(image_url, headers=headers)

                                    if image_response.status_code == 200:
                                        download_url = image_response.json().get('download_url')
                                        logger.debug(f"download_url: {download_url}")
                                        if USE_OAIUSERCONTENT_URL == True:
                                            if ((BOT_MODE_ENABLED == False) or (
                                                    BOT_MODE_ENABLED == True and BOT_MODE_ENABLED_MARKDOWN_IMAGE_OUTPUT == True)):
                                                new_text = f"\n![image]({download_url})\n[ä¸‹è½½é“¾æ¥]({download_url})\n"
                                            if BOT_MODE_ENABLED == True and BOT_MODE_ENABLED_PLAIN_IMAGE_URL_OUTPUT == True:
                                                if all_new_text != "":
                                                    new_text = f"\nå›¾ç‰‡é“¾æ¥ï¼š{download_url}\n"
                                                else:
                                                    new_text = f"å›¾ç‰‡é“¾æ¥ï¼š{download_url}\n"
                                        else:
                                            # ä»URLä¸‹è½½å›¾ç‰‡
                                            # image_data = requests.get(download_url).content
                                            image_download_response = requests.get(download_url)
                                            # print(f"image_download_response: {image_download_response.text}")
                                            if image_download_response.status_code == 200:
                                                logger.debug(f"ä¸‹è½½å›¾ç‰‡æˆåŠŸ")
                                                image_data = image_download_response.content
                                                today_image_url = save_image(image_data)  # ä¿å­˜å›¾ç‰‡ï¼Œå¹¶è·å–æ–‡ä»¶å
                                                if ((BOT_MODE_ENABLED == False) or (
                                                        BOT_MODE_ENABLED == True and BOT_MODE_ENABLED_MARKDOWN_IMAGE_OUTPUT == True)):
                                                    new_text = f"\n![image]({UPLOAD_BASE_URL}/{today_image_url})\n[ä¸‹è½½é“¾æ¥]({UPLOAD_BASE_URL}/{today_image_url})\n"
                                                if BOT_MODE_ENABLED == True and BOT_MODE_ENABLED_PLAIN_IMAGE_URL_OUTPUT == True:
                                                    if all_new_text != "":
                                                        new_text = f"\nå›¾ç‰‡é“¾æ¥ï¼š{UPLOAD_BASE_URL}/{today_image_url}\n"
                                                    else:
                                                        new_text = f"å›¾ç‰‡é“¾æ¥ï¼š{UPLOAD_BASE_URL}/{today_image_url}\n"
                                            else:
                                                logger.error(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {image_download_response.text}")
                                        if last_content_type == "code":
                                            if BOT_MODE_ENABLED and BOT_MODE_ENABLED_CODE_BLOCK_OUTPUT == False:
                                                new_text = new_text
                                            else:
                                                new_text = "\n```\n" + new_text

                                        logger.debug(f"new_text: {new_text}")
                                        is_img_message = True
                                    else:
                                        logger.error(f"è·å–å›¾ç‰‡ä¸‹è½½é“¾æ¥å¤±è´¥: {image_response.text}")
                            except:
                                pass

                        if is_img_message == False:
                            # print(f"data_json: {data_json}")
                            if content_type == "multimodal_text" and last_content_type == "code":
                                new_text = "\n```\n" + content.get("text", "")
                                if BOT_MODE_ENABLED and BOT_MODE_ENABLED_CODE_BLOCK_OUTPUT == False:
                                    new_text = content.get("text", "")
                            elif role == "tool" and name == "dalle.text2im":
                                logger.debug(f"æ— è§†æ¶ˆæ¯: {content.get('text', '')}")
                                continue
                            # ä»£ç å—ç‰¹æ®Šå¤„ç†
                            if content_type == "code" and last_content_type != "code" and content_type != None:
                                full_code = ''.join(content.get("text", ""))
                                new_text = "\n```\n" + full_code[len(last_full_code):]
                                # print(f"full_code: {full_code}")
                                # print(f"last_full_code: {last_full_code}")
                                # print(f"new_text: {new_text}")
                                last_full_code = full_code  # æ›´æ–°å®Œæ•´ä»£ç ä»¥å¤‡ä¸‹æ¬¡æ¯”è¾ƒ
                                if BOT_MODE_ENABLED and BOT_MODE_ENABLED_CODE_BLOCK_OUTPUT == False:
                                    new_text = ""

                            elif last_content_type == "code" and content_type != "code" and content_type != None:
                                full_code = ''.join(content.get("text", ""))
                                new_text = "\n```\n" + full_code[len(last_full_code):]
                                # print(f"full_code: {full_code}")
                                # print(f"last_full_code: {last_full_code}")
                                # print(f"new_text: {new_text}")
                                last_full_code = ""  # æ›´æ–°å®Œæ•´ä»£ç ä»¥å¤‡ä¸‹æ¬¡æ¯”è¾ƒ
                                if BOT_MODE_ENABLED and BOT_MODE_ENABLED_CODE_BLOCK_OUTPUT == False:
                                    new_text = ""

                            elif content_type == "code" and last_content_type == "code" and content_type != None:
                                full_code = ''.join(content.get("text", ""))
                                new_text = full_code[len(last_full_code):]
                                # print(f"full_code: {full_code}")
                                # print(f"last_full_code: {last_full_code}")
                                # print(f"new_text: {new_text}")
                                last_full_code = full_code  # æ›´æ–°å®Œæ•´ä»£ç ä»¥å¤‡ä¸‹æ¬¡æ¯”è¾ƒ
                                if BOT_MODE_ENABLED and BOT_MODE_ENABLED_CODE_BLOCK_OUTPUT == False:
                                    new_text = ""

                            else:
                                # åªè·å–æ–°çš„ parts
                                parts = content.get("parts", [])
                                full_text = ''.join(parts)
                                if full_text == "![":
                                    last_full_text = "!"
                                new_text = full_text[len(last_full_text):]
                                last_full_text = full_text
                                if "\u3010" in new_text and not citation_accumulating:
                                    citation_accumulating = True
                                    citation_buffer = citation_buffer + new_text
                                    # print(f"å¼€å§‹ç§¯ç´¯å¼•ç”¨: {citation_buffer}")
                                elif citation_accumulating:
                                    citation_buffer += new_text
                                    # print(f"ç§¯ç´¯å¼•ç”¨: {citation_buffer}")
                                if citation_accumulating:
                                    if is_valid_citation_format(citation_buffer):
                                        # print(f"åˆæ³•æ ¼å¼: {citation_buffer}")
                                        # ç»§ç»­ç§¯ç´¯
                                        if is_complete_citation_format(citation_buffer):

                                            # æ›¿æ¢å®Œæ•´çš„å¼•ç”¨æ ¼å¼
                                            replaced_text, remaining_text, is_potential_citation = replace_complete_citation(
                                                citation_buffer, citations)
                                            # print(replaced_text)  # è¾“å‡ºæ›¿æ¢åçš„æ–‡æœ¬
                                            new_text = replaced_text

                                            if (is_potential_citation):
                                                citation_buffer = remaining_text
                                            else:
                                                citation_accumulating = False
                                                citation_buffer = ""
                                            # print(f"æ›¿æ¢å®Œæ•´çš„å¼•ç”¨æ ¼å¼: {new_text}")
                                        else:
                                            continue
                                    else:
                                        # ä¸æ˜¯åˆæ³•æ ¼å¼ï¼Œæ”¾å¼ƒç§¯ç´¯å¹¶å“åº”
                                        # print(f"ä¸åˆæ³•æ ¼å¼: {citation_buffer}")
                                        new_text = citation_buffer
                                        citation_accumulating = False
                                        citation_buffer = ""

                                if "(" in new_text and not file_output_accumulating and not citation_accumulating:
                                    file_output_accumulating = True
                                    file_output_buffer = file_output_buffer + new_text
                                    logger.debug(f"å¼€å§‹ç§¯ç´¯æ–‡ä»¶è¾“å‡º: {file_output_buffer}")
                                elif file_output_accumulating:
                                    file_output_buffer += new_text
                                    logger.debug(f"ç§¯ç´¯æ–‡ä»¶è¾“å‡º: {file_output_buffer}")
                                if file_output_accumulating:
                                    if is_valid_sandbox_combined_corrected_final_v2(file_output_buffer):
                                        logger.debug(f"åˆæ³•æ–‡ä»¶è¾“å‡ºæ ¼å¼: {file_output_buffer}")
                                        # ç»§ç»­ç§¯ç´¯
                                        if is_complete_sandbox_format(file_output_buffer):
                                            # æ›¿æ¢å®Œæ•´çš„å¼•ç”¨æ ¼å¼
                                            replaced_text = replace_sandbox(file_output_buffer, conversation_id,
                                                                            message_id, api_key, proxy_api_prefix)
                                            # print(replaced_text)  # è¾“å‡ºæ›¿æ¢åçš„æ–‡æœ¬
                                            new_text = replaced_text
                                            file_output_accumulating = False
                                            file_output_buffer = ""
                                            logger.debug(f"æ›¿æ¢å®Œæ•´çš„æ–‡ä»¶è¾“å‡ºæ ¼å¼: {new_text}")
                                        else:
                                            continue
                                    else:
                                        # ä¸æ˜¯åˆæ³•æ ¼å¼ï¼Œæ”¾å¼ƒç§¯ç´¯å¹¶å“åº”
                                        logger.debug(f"ä¸åˆæ³•æ ¼å¼: {file_output_buffer}")
                                        new_text = file_output_buffer
                                        file_output_accumulating = False
                                        file_output_buffer = ""

                            # Python å·¥å…·æ‰§è¡Œè¾“å‡ºç‰¹æ®Šå¤„ç†
                            if role == "tool" and name == "python" and last_content_type != "execution_output" and content_type != None:
                                full_code_result = ''.join(content.get("text", ""))
                                new_text = "`Result:` \n```\n" + full_code_result[len(last_full_code_result):]
                                if last_content_type == "code":
                                    if BOT_MODE_ENABLED and BOT_MODE_ENABLED_CODE_BLOCK_OUTPUT == False:
                                        new_text = ""
                                    else:
                                        new_text = "\n```\n" + new_text
                                # print(f"full_code_result: {full_code_result}")
                                # print(f"last_full_code_result: {last_full_code_result}")
                                # print(f"new_text: {new_text}")
                                last_full_code_result = full_code_result  # æ›´æ–°å®Œæ•´ä»£ç ä»¥å¤‡ä¸‹æ¬¡æ¯”è¾ƒ
                            elif last_content_type == "execution_output" and (
                                    role != "tool" or name != "python") and content_type != None:
                                # new_text = content.get("text", "") + "\n```"
                                full_code_result = ''.join(content.get("text", ""))
                                new_text = full_code_result[len(last_full_code_result):] + "\n```\n"
                                if BOT_MODE_ENABLED and BOT_MODE_ENABLED_CODE_BLOCK_OUTPUT == False:
                                    new_text = ""
                                tmp_new_text = new_text
                                if execution_output_image_url_buffer != "":
                                    if ((BOT_MODE_ENABLED == False) or (
                                            BOT_MODE_ENABLED == True and BOT_MODE_ENABLED_MARKDOWN_IMAGE_OUTPUT == True)):
                                        logger.debug(f"BOT_MODE_ENABLED: {BOT_MODE_ENABLED}")
                                        logger.debug(
                                            f"BOT_MODE_ENABLED_MARKDOWN_IMAGE_OUTPUT: {BOT_MODE_ENABLED_MARKDOWN_IMAGE_OUTPUT}")
                                        new_text = tmp_new_text + f"![image]({execution_output_image_url_buffer})\n[ä¸‹è½½é“¾æ¥]({execution_output_image_url_buffer})\n"
                                    if BOT_MODE_ENABLED == True and BOT_MODE_ENABLED_PLAIN_IMAGE_URL_OUTPUT == True:
                                        logger.debug(f"BOT_MODE_ENABLED: {BOT_MODE_ENABLED}")
                                        logger.debug(
                                            f"BOT_MODE_ENABLED_PLAIN_IMAGE_URL_OUTPUT: {BOT_MODE_ENABLED_PLAIN_IMAGE_URL_OUTPUT}")
                                        new_text = tmp_new_text + f"å›¾ç‰‡é“¾æ¥ï¼š{execution_output_image_url_buffer}\n"
                                    execution_output_image_url_buffer = ""

                                if content_type == "code":
                                    new_text = new_text + "\n```\n"
                                # print(f"full_code_result: {full_code_result}")
                                # print(f"last_full_code_result: {last_full_code_result}")
                                # print(f"new_text: {new_text}")
                                last_full_code_result = ""  # æ›´æ–°å®Œæ•´ä»£ç ä»¥å¤‡ä¸‹æ¬¡æ¯”è¾ƒ
                            elif last_content_type == "execution_output" and role == "tool" and name == "python" and content_type != None:
                                full_code_result = ''.join(content.get("text", ""))
                                if BOT_MODE_ENABLED and BOT_MODE_ENABLED_CODE_BLOCK_OUTPUT == False:
                                    new_text = ""
                                else:
                                    new_text = full_code_result[len(last_full_code_result):]
                                # print(f"full_code_result: {full_code_result}")
                                # print(f"last_full_code_result: {last_full_code_result}")
                                # print(f"new_text: {new_text}")
                                last_full_code_result = full_code_result

                            # å…¶ä½™Actionæ‰§è¡Œè¾“å‡ºç‰¹æ®Šå¤„ç†
                            # if role == "tool" and name != "python" and name != "dalle.text2im" and last_content_type != "execution_output" and content_type != None:
                            #     new_text = ""
                            #     if last_content_type == "code":
                            #         if BOT_MODE_ENABLED and BOT_MODE_ENABLED_CODE_BLOCK_OUTPUT == False:
                            #             new_text = ""
                            #         else:
                            #             new_text = "\n```\n" + new_text

                        # æ£€æŸ¥ new_text ä¸­æ˜¯å¦åŒ…å« <<ImageDisplayed>>
                        if "<<ImageDisplayed>>" in last_full_code_result:
                            # è¿›è¡Œæå–æ“ä½œ
                            aggregate_result = message.get("metadata", {}).get("aggregate_result", {})
                            if aggregate_result:
                                messages = aggregate_result.get("messages", [])
                                for msg in messages:
                                    if msg.get("message_type") == "image":
                                        image_url = msg.get("image_url")
                                        if image_url:
                                            # ä» image_url æå–æ‰€éœ€çš„å­—æ®µ
                                            image_file_id = image_url.split('://')[-1]
                                            logger.info(f"æå–åˆ°çš„å›¾ç‰‡æ–‡ä»¶ID: {image_file_id}")
                                            if image_file_id != execution_output_image_id_buffer:
                                                image_url = f"{BASE_URL}{proxy_api_prefix}/backend-api/files/{image_file_id}/download"

                                                headers = {
                                                    "Authorization": f"Bearer {api_key}"
                                                }
                                                image_response = requests.get(image_url, headers=headers)

                                                if image_response.status_code == 200:
                                                    download_url = image_response.json().get('download_url')
                                                    logger.debug(f"download_url: {download_url}")
                                                    if USE_OAIUSERCONTENT_URL == True:
                                                        execution_output_image_url_buffer = download_url

                                                    else:
                                                        # ä»URLä¸‹è½½å›¾ç‰‡
                                                        # image_data = requests.get(download_url).content
                                                        image_download_response = requests.get(download_url)
                                                        # print(f"image_download_response: {image_download_response.text}")
                                                        if image_download_response.status_code == 200:
                                                            logger.debug(f"ä¸‹è½½å›¾ç‰‡æˆåŠŸ")
                                                            image_data = image_download_response.content
                                                            today_image_url = save_image(image_data)  # ä¿å­˜å›¾ç‰‡ï¼Œå¹¶è·å–æ–‡ä»¶å
                                                            execution_output_image_url_buffer = f"{UPLOAD_BASE_URL}/{today_image_url}"

                                                        else:
                                                            logger.error(
                                                                f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {image_download_response.text}")

                                            execution_output_image_id_buffer = image_file_id

                        # ä» new_text ä¸­ç§»é™¤ <<ImageDisplayed>>
                        new_text = new_text.replace(
                            "All the files uploaded by the user have been fully loaded. Searching won't provide "
                            "additional information.",
                            UPLOAD_SUCCESS_TEXT)
                        new_text = new_text.replace("<<ImageDisplayed>>", "å›¾ç‰‡ç”Ÿæˆä¸­ï¼Œè¯·ç¨å\n")

                        # print(f"æ”¶åˆ°æ•°æ®: {data_json}")
                        # print(f"æ”¶åˆ°çš„å®Œæ•´æ–‡æœ¬: {full_text}")
                        # print(f"ä¸Šæ¬¡æ”¶åˆ°çš„å®Œæ•´æ–‡æœ¬: {last_full_text}")
                        # print(f"æ–°çš„æ–‡æœ¬: {new_text}")

                        # æ›´æ–° last_content_type
                        if content_type != None:
                            last_content_type = content_type if role != "user" else last_content_type

                        model_slug = message.get("metadata", {}).get("model_slug") or model

                        if first_output:
                            new_data = {
                                "id": chat_message_id,
                                "object": "chat.completion.chunk",
                                "created": timestamp,
                                "model": model_slug,
                                "choices": [
                                    {
                                        "index": 0,
                                        "delta": {"role": "assistant"},
                                        "finish_reason": None
                                    }
                                ]
                            }
                            q_data = 'data: ' + json.dumps(new_data, ensure_ascii=False) + '\n\n'
                            data_queue.put(q_data)
                            first_output = False

                        new_data = {
                            "id": chat_message_id,
                            "object": "chat.completion.chunk",
                            "created": timestamp,
                            "model": model_slug,
                            "choices": [
                                {
                                    "index": 0,
                                    "delta": {
                                        "content": ''.join(new_text)
                                    },
                                    "finish_reason": None
                                }
                            ]
                        }
                        # print(f"Role: {role}")
                        logger.info(f"å‘é€æ¶ˆæ¯: {new_text}")
                        tmp = 'data: ' + json.dumps(new_data, ensure_ascii=False) + '\n\n'
                        # print(f"å‘é€æ•°æ®: {tmp}")
                        # ç´¯ç§¯ new_text
                        all_new_text += new_text
                        q_data = 'data: ' + json.dumps(new_data, ensure_ascii=False) + '\n\n'
                        data_queue.put(q_data)
                        last_data_time[0] = time.time()
                        if stop_event.is_set():
                            break
                    except json.JSONDecodeError:
                        # print("JSON è§£æé”™è¯¯")
                        logger.info(f"å‘é€æ•°æ®: {complete_data}")
                        if complete_data == 'data: [DONE]\n\n':
                            logger.info(f"ä¼šè¯ç»“æŸ")
                            q_data = complete_data
                            data_queue.put(('all_new_text', all_new_text))
                            data_queue.put(q_data)
                            last_data_time[0] = time.time()
                            if stop_event.is_set():
                                break
        if citation_buffer != "":
            new_data = {
                "id": chat_message_id,
                "object": "chat.completion.chunk",
                "created": timestamp,
                "model": message.get("metadata", {}).get("model_slug"),
                "choices": [
                    {
                        "index": 0,
                        "delta": {
                            "content": ''.join(citation_buffer)
                        },
                        "finish_reason": None
                    }
                ]
            }
            tmp = 'data: ' + json.dumps(new_data) + '\n\n'
            # print(f"å‘é€æ•°æ®: {tmp}")
            # ç´¯ç§¯ new_text
            all_new_text += citation_buffer
            q_data = 'data: ' + json.dumps(new_data) + '\n\n'
            data_queue.put(q_data)
            last_data_time[0] = time.time()
        if buffer:
            try:
                buffer_json = json.loads(buffer)
                logger.info(f"æœ€åçš„ç¼“å­˜æ•°æ®: {buffer_json}")
                error_message = buffer_json.get("detail", {}).get("message", "æœªçŸ¥é”™è¯¯")
                error_data = {
                    "id": chat_message_id,
                    "object": "chat.completion.chunk",
                    "created": timestamp,
                    "model": "error",
                    "choices": [
                        {
                            "index": 0,
                            "delta": {
                                "content": ''.join("```\n" + error_message + "\n```")
                            },
                            "finish_reason": None
                        }
                    ]
                }
                tmp = 'data: ' + json.dumps(error_data) + '\n\n'
                logger.info(f"å‘é€æœ€åçš„æ•°æ®: {tmp}")
                # ç´¯ç§¯ new_text
                all_new_text += ''.join("```\n" + error_message + "\n```")
                q_data = 'data: ' + json.dumps(error_data) + '\n\n'
                data_queue.put(q_data)
                last_data_time[0] = time.time()
                complete_data = 'data: [DONE]\n\n'
                logger.info(f"ä¼šè¯ç»“æŸ")
                q_data = complete_data
                data_queue.put(('all_new_text', all_new_text))
                data_queue.put(q_data)
                last_data_time[0] = time.time()
            except:
                # print("JSON è§£æé”™è¯¯")
                logger.info(f"å‘é€æœ€åçš„æ•°æ®: {buffer}")
                error_data = {
                    "id": chat_message_id,
                    "object": "chat.completion.chunk",
                    "created": timestamp,
                    "model": "error",
                    "choices": [
                        {
                            "index": 0,
                            "delta": {
                                "content": ''.join("```\n" + buffer + "\n```")
                            },
                            "finish_reason": None
                        }
                    ]
                }
                tmp = 'data: ' + json.dumps(error_data) + '\n\n'
                q_data = tmp
                data_queue.put(q_data)
                last_data_time[0] = time.time()
                complete_data = 'data: [DONE]\n\n'
                logger.info(f"ä¼šè¯ç»“æŸ")
                q_data = complete_data
                data_queue.put(('all_new_text', all_new_text))
                data_queue.put(q_data)
                last_data_time[0] = time.time()
    except Exception as e:
        logger.error(f"Exception: {e}")
        complete_data = 'data: [DONE]\n\n'
        logger.info(f"ä¼šè¯ç»“æŸ")
        q_data = complete_data
        data_queue.put(('all_new_text', all_new_text))
        data_queue.put(q_data)
        last_data_time[0] = time.time()


def keep_alive(last_data_time, stop_event, queue, model, chat_message_id):
    while not stop_event.is_set():
        if time.time() - last_data_time[0] >= 1:
            # logger.debug(f"å‘é€ä¿æ´»æ¶ˆæ¯")
            # å½“å‰æ—¶é—´æˆ³
            timestamp = int(time.time())
            new_data = {
                "id": chat_message_id,
                "object": "chat.completion.chunk",
                "created": timestamp,
                "model": model,
                "choices": [
                    {
                        "index": 0,
                        "delta": {
                            "content": ''
                        },
                        "finish_reason": None
                    }
                ]
            }
            queue.put(f'data: {json.dumps(new_data)}\n\n')  # å‘é€ä¿æ´»æ¶ˆæ¯
            last_data_time[0] = time.time()
        time.sleep(1)

    if stop_event.is_set():
        logger.debug(f"æ¥å—åˆ°åœæ­¢ä¿¡å·ï¼Œåœæ­¢ä¿æ´»çº¿ç¨‹")
        return


import tiktoken


def count_tokens(text, model_name):
    """
    Count the number of tokens for a given text using a specified model.

    :param text: The text to be tokenized.
    :param model_name: The name of the model to use for tokenization.
    :return: Number of tokens in the text for the specified model.
    """
    # è·å–æŒ‡å®šæ¨¡å‹çš„ç¼–ç å™¨
    if model_name == 'gpt-3.5-turbo':
        model_name = 'gpt-3.5-turbo'
    else:
        model_name = 'gpt-4'
    encoder = tiktoken.encoding_for_model(model_name)

    # ç¼–ç æ–‡æœ¬å¹¶è®¡ç®—tokenæ•°é‡
    token_list = encoder.encode(text)
    return len(token_list)


def count_total_input_words(messages, model):
    """
    Count the total number of words in all messages' content.
    """
    total_words = 0
    for message in messages:
        content = message.get("content", "")
        if isinstance(content, list):  # åˆ¤æ–­contentæ˜¯å¦ä¸ºåˆ—è¡¨
            for item in content:
                if item.get("type") == "text":  # ä»…å¤„ç†ç±»å‹ä¸º"text"çš„é¡¹
                    text_content = item.get("text", "")
                    total_words += count_tokens(text_content, model)
        elif isinstance(content, str):  # å¤„ç†å­—ç¬¦ä¸²ç±»å‹çš„content
            total_words += count_tokens(content, model)
        # ä¸å¤„ç†å…¶ä»–ç±»å‹çš„content

    return total_words


# æ·»åŠ ç¼“å­˜
def add_to_dict(key, value):
    global refresh_dict
    refresh_dict[key] = value
    logger.info("æ·»åŠ access_tokenç¼“å­˜æˆåŠŸ.............")


import threading
import time


# å®šä¹‰ Flask è·¯ç”±
@app.route(f'/{API_PREFIX}/v1/chat/completions' if API_PREFIX else '/v1/chat/completions', methods=['POST'])
def chat_completions():
    logger.info(f"New Request")
    proxy_api_prefix = getPROXY_API_PREFIX(lock)

    if proxy_api_prefix == None:
        return jsonify({"error": "PROXY_API_PREFIX is not accessible"}), 401
    data = request.json
    messages = data.get('messages')
    model = data.get('model', "gpt-3.5-turbo")
    ori_model_name = model
    accessible_model_list = get_accessible_model_list()
    if model not in accessible_model_list and not 'gpt-4-gizmo-' in model:
        return jsonify({"error": "model is not accessible"}), 401
    model_config = find_model_config(model)
    if model_config:
        ori_model_name = model_config.get('ori_name', model)
    if "o1-" in ori_model_name:
        # ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼è¿‡æ»¤ç³»ç»Ÿè§’è‰²
        messages = [message for message in messages if message["role"] in ["user", "assistant"]]

    stream = data.get('stream', False)

    auth_header = request.headers.get('Authorization')
    if not auth_header or not auth_header.startswith('Bearer '):
        return jsonify({"error": "Authorization header is missing or invalid"}), 401
    api_key = None
    try:
        api_key = auth_header.split(' ')[1].split(',')[0].strip()
        account_id = auth_header.split(' ')[1].split(',')[1].strip()
        logging.info(f"{api_key}:{account_id}")
    except IndexError:
        account_id = None
    if not api_key.startswith("eyJhb"):
        refresh_token = api_key
        if api_key in refresh_dict:
            logger.info(f"ä»ç¼“å­˜è¯»å–åˆ°api_key.........ã€‚")
            api_key = refresh_dict.get(api_key)
        else:
            if REFRESH_TOACCESS_ENABLEOAI:
                api_key = oaiGetAccessToken(api_key)
            else:
                api_key = oaiFreeGetAccessToken(REFRESH_TOACCESS_OAIFREE_REFRESHTOACCESS_URL, api_key)
            if not api_key.startswith("eyJhb"):
                return jsonify({"error": "refresh_token is wrong or refresh_token url is wrong!"}), 401
            add_to_dict(refresh_token, api_key)
    logger.info(f"api_key: {api_key}")

    upstream_response = send_text_prompt_and_get_response(messages, api_key, account_id, stream, model,
                                                          proxy_api_prefix)

    if upstream_response.status_code != 200:
        return jsonify({"error": f"{upstream_response.text}"}), upstream_response.status_code

    # åœ¨éæµå¼å“åº”çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå˜é‡æ¥ç´¯ç§¯æ‰€æœ‰çš„ new_text
    all_new_text = ""

    # å¤„ç†æµå¼å“åº”
    def generate(proxy_api_prefix):
        nonlocal all_new_text  # å¼•ç”¨å¤–éƒ¨å˜é‡
        data_queue = Queue()
        stop_event = threading.Event()
        last_data_time = [time.time()]
        chat_message_id = generate_unique_id("chatcmpl")

        conversation_id_print_tag = False

        conversation_id = ''

        # å¯åŠ¨æ•°æ®å¤„ç†çº¿ç¨‹
        fetcher_thread = threading.Thread(target=data_fetcher, args=(
            upstream_response, data_queue, stop_event, last_data_time, api_key, chat_message_id, model,
            proxy_api_prefix))
        fetcher_thread.start()

        # å¯åŠ¨ä¿æ´»çº¿ç¨‹
        keep_alive_thread = threading.Thread(target=keep_alive,
                                             args=(last_data_time, stop_event, data_queue, model, chat_message_id))
        keep_alive_thread.start()

        try:
            while True:
                data = data_queue.get()
                if isinstance(data, tuple) and data[0] == 'all_new_text':
                    # æ›´æ–° all_new_text
                    logger.info(f"å®Œæ•´æ¶ˆæ¯: {data[1]}")
                    all_new_text += data[1]
                elif isinstance(data, tuple) and data[0] == 'conversation_id':
                    if conversation_id_print_tag == False:
                        logger.info(f"å½“å‰ä¼šè¯id: {data[1]}")
                        conversation_id_print_tag = True
                    # æ›´æ–° conversation_id
                    conversation_id = data[1]
                    # print(f"æ”¶åˆ°ä¼šè¯id: {conversation_id}")
                elif data == 'data: [DONE]\n\n':
                    # æ¥æ”¶åˆ°ç»“æŸä¿¡å·ï¼Œé€€å‡ºå¾ªç¯
                    timestamp = int(time.time())

                    new_data = {
                        "id": chat_message_id,
                        "object": "chat.completion.chunk",
                        "created": timestamp,
                        "model": model,
                        "choices": [
                            {
                                "delta": {},
                                "index": 0,
                                "finish_reason": "stop"
                            }
                        ]
                    }
                    q_data = 'data: ' + json.dumps(new_data, ensure_ascii=False) + '\n\n'
                    yield q_data

                    logger.debug(f"ä¼šè¯ç»“æŸ-å¤–å±‚")
                    yield data
                    break
                else:
                    yield data

                # STEAM_SLEEP_TIME ä¼˜åŒ–ä¼ è¾“è´¨é‡ï¼Œæ”¹å–„å¡é¡¿ç°è±¡
                if stream and STEAM_SLEEP_TIME > 0:
                    time.sleep(STEAM_SLEEP_TIME)

        finally:
            stop_event.set()
            fetcher_thread.join()
            keep_alive_thread.join()

            # if conversation_id:
            #     # print(f"å‡†å¤‡åˆ é™¤çš„ä¼šè¯idï¼š {conversation_id}")
            #     delete_conversation(conversation_id, api_key,proxy_api_prefix)

    if not stream:
        # æ‰§è¡Œæµå¼å“åº”çš„ç”Ÿæˆå‡½æ•°æ¥ç´¯ç§¯ all_new_text
        # è¿­ä»£ç”Ÿæˆå™¨å¯¹è±¡ä»¥æ‰§è¡Œå…¶å†…éƒ¨é€»è¾‘
        for _ in generate(proxy_api_prefix):
            pass
        # æ„é€ å“åº”çš„ JSON ç»“æ„
        ori_model_name = ''
        model_config = find_model_config(model)
        if model_config:
            ori_model_name = model_config.get('ori_name', model)
        input_tokens = count_total_input_words(messages, ori_model_name)
        comp_tokens = count_tokens(all_new_text, ori_model_name)
        if input_tokens >= 100 and comp_tokens <= 0:
            # è¿”å›é”™è¯¯æ¶ˆæ¯å’ŒçŠ¶æ€ç 429
            error_response = {"error": "ç©ºå›å¤"}
            return jsonify(error_response), 429
        else:
            response_json = {
                "id": generate_unique_id("chatcmpl"),
                "object": "chat.completion",
                "created": int(time.time()),  # ä½¿ç”¨å½“å‰æ—¶é—´æˆ³
                "model": model,  # ä½¿ç”¨è¯·æ±‚ä¸­æŒ‡å®šçš„æ¨¡å‹
                "choices": [
                    {
                        "index": 0,
                        "message": {
                            "role": "assistant",
                            "content": all_new_text  # ä½¿ç”¨ç´¯ç§¯çš„æ–‡æœ¬
                        },
                        "finish_reason": "stop"
                    }
                ],
                "usage": {
                    # è¿™é‡Œçš„ token è®¡æ•°éœ€è¦æ ¹æ®å®é™…æƒ…å†µè®¡ç®—
                    "prompt_tokens": input_tokens,
                    "completion_tokens": comp_tokens,
                    "total_tokens": input_tokens + comp_tokens
                },
                "system_fingerprint": None
            }
            # è¿”å› JSON å“åº”
            return jsonify(response_json)
    else:
        return Response(generate(proxy_api_prefix), mimetype='text/event-stream')


@app.route(f'/{API_PREFIX}/v1/images/generations' if API_PREFIX else '/v1/images/generations', methods=['POST'])
def images_generations():
    logger.info(f"New Img Request")
    proxy_api_prefix = getPROXY_API_PREFIX(lock)
    if proxy_api_prefix == None:
        return jsonify({"error": "PROXY_API_PREFIX is not accessible"}), 401
    data = request.json
    logger.debug(f"data: {data}")
    api_key = None
    model = data.get('model', "gpt-3.5-turbo")
    ori_model_name = model
    accessible_model_list = get_accessible_model_list()
    if model not in accessible_model_list and not 'gpt-4-gizmo-' in model:
        return jsonify({"error": "model is not accessible"}), 401
    model_config = find_model_config(model)
    if model_config:
        ori_model_name = model_config.get('ori_name', model)
    if "o1-" in ori_model_name:
        # ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼è¿‡æ»¤ç³»ç»Ÿè§’è‰²
        messages = [message for message in messages if message["role"] in ["user", "assistant"]]
    # è·å–è¯·æ±‚ä¸­çš„response_formatå‚æ•°ï¼Œé»˜è®¤ä¸º"url"
    response_format = data.get('response_format', 'url')
    # è·å–è¯·æ±‚ä¸­çš„sizeå‚æ•°ï¼Œé»˜è®¤ä¸º"1024x1024"
    response_size = data.get('size', '1024x1024')

    prompt = data.get('prompt', '')

    prompt = DALLE_PROMPT_PREFIX + '\nprompt:' + prompt + '\nsize:' + response_size

    # stream = data.get('stream', False)

    auth_header = request.headers.get('Authorization')
    if not auth_header or not auth_header.startswith('Bearer '):
        return jsonify({"error": "Authorization header is missing or invalid"}), 401
    try:
        api_key = auth_header.split(' ')[1].split(',')[0].strip()
        account_id = auth_header.split(' ')[1].split(',')[1].strip()
        logging.info(f"{api_key}:{account_id}")
    except IndexError:
        account_id = None
    if not api_key.startswith("eyJhb"):
        refresh_token = api_key
        if api_key in refresh_dict:
            logger.info(f"ä»ç¼“å­˜è¯»å–åˆ°api_key.........")
            api_key = refresh_dict.get(api_key)
        else:
            if REFRESH_TOACCESS_ENABLEOAI:
                api_key = oaiGetAccessToken(api_key)
            else:
                api_key = oaiFreeGetAccessToken(REFRESH_TOACCESS_OAIFREE_REFRESHTOACCESS_URL, api_key)
            if not api_key.startswith("eyJhb"):
                return jsonify({"error": "refresh_token is wrong or refresh_token url is wrong!"}), 401
            add_to_dict(refresh_token, api_key)

    logger.info(f"api_key: {api_key}")

    image_urls = []

    messages = [
        {
            "role": "user",
            "content": prompt,
            "hasName": False
        }
    ]

    upstream_response = send_text_prompt_and_get_response(messages, api_key, account_id, False, model, proxy_api_prefix)

    if upstream_response.status_code != 200:
        return jsonify({"error": f"{upstream_response.text}"}), upstream_response.status_code

    # åœ¨éæµå¼å“åº”çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå˜é‡æ¥ç´¯ç§¯æ‰€æœ‰çš„ new_text
    all_new_text = ""

    # å¤„ç†æµå¼å“åº”
    def generate(proxy_api_prefix):
        nonlocal all_new_text  # å¼•ç”¨å¤–éƒ¨å˜é‡
        chat_message_id = generate_unique_id("chatcmpl")
        # å½“å‰æ—¶é—´æˆ³
        timestamp = int(time.time())

        buffer = ""
        last_full_text = ""  # ç”¨äºå­˜å‚¨ä¹‹å‰æ‰€æœ‰å‡ºç°è¿‡çš„ parts ç»„æˆçš„å®Œæ•´æ–‡æœ¬
        last_full_code = ""
        last_full_code_result = ""
        last_content_type = None  # ç”¨äºè®°å½•ä¸Šä¸€ä¸ªæ¶ˆæ¯çš„å†…å®¹ç±»å‹
        conversation_id = ''
        citation_buffer = ""
        citation_accumulating = False
        message = None
        for chunk in upstream_response.iter_content(chunk_size=1024):
            if chunk:
                buffer += chunk.decode('utf-8')
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ "event: ping"ï¼Œå¦‚æœå­˜åœ¨ï¼Œåˆ™åªä¿ç•™ "data:" åé¢çš„å†…å®¹
                if "event: ping" in buffer:
                    if "data:" in buffer:
                        buffer = buffer.split("data:", 1)[1]
                        buffer = "data:" + buffer
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç§»é™¤ç‰¹å®šæ ¼å¼çš„å­—ç¬¦ä¸²
                # print("åº”ç”¨æ­£åˆ™è¡¨è¾¾å¼ä¹‹å‰çš„ buffer:", buffer.replace('\n', '\\n'))
                buffer = re.sub(r'data: \d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d{6}(\r\n|\r|\n){2}', '', buffer)
                # print("åº”ç”¨æ­£åˆ™è¡¨è¾¾å¼ä¹‹åçš„ buffer:", buffer.replace('\n', '\\n'))

                while 'data:' in buffer and '\n\n' in buffer:
                    end_index = buffer.index('\n\n') + 2
                    complete_data, buffer = buffer[:end_index], buffer[end_index:]
                    # è§£æ data å—
                    try:
                        data_json = json.loads(complete_data.replace('data: ', ''))
                        # print(f"data_json: {data_json}")
                        message = data_json.get("message", {})

                        if message is None:
                            logger.error(f"message ä¸ºç©º: data_json: {data_json}")

                        message_status = message.get("status")
                        content = message.get("content", {})
                        role = message.get("author", {}).get("role")
                        content_type = content.get("content_type")
                        # logger.debug(f"content_type: {content_type}")
                        # logger.debug(f"last_content_type: {last_content_type}")

                        metadata = {}
                        citations = []
                        try:
                            metadata = message.get("metadata", {})
                            citations = metadata.get("citations", [])
                        except:
                            pass
                        name = message.get("author", {}).get("name")
                        if (
                                role == "user" or message_status == "finished_successfully" or role == "system") and role != "tool":
                            # å¦‚æœæ˜¯ç”¨æˆ·å‘æ¥çš„æ¶ˆæ¯ï¼Œç›´æ¥èˆå¼ƒ
                            continue
                        try:
                            conversation_id = data_json.get("conversation_id")
                            logger.debug(f"conversation_id: {conversation_id}")
                        except:
                            pass
                            # åªè·å–æ–°çš„éƒ¨åˆ†
                        new_text = ""
                        is_img_message = False
                        parts = content.get("parts", [])
                        for part in parts:
                            try:
                                # print(f"part: {part}")
                                # print(f"part type: {part.get('content_type')}")
                                if part.get('content_type') == 'image_asset_pointer':
                                    logger.debug(f"find img message~")
                                    is_img_message = True
                                    asset_pointer = part.get('asset_pointer').replace('file-service://', '')
                                    logger.debug(f"asset_pointer: {asset_pointer}")
                                    image_url = f"{BASE_URL}{proxy_api_prefix}/backend-api/files/{asset_pointer}/download"

                                    headers = {
                                        "Authorization": f"Bearer {api_key}"
                                    }
                                    image_response = requests.get(image_url, headers=headers)

                                    if image_response.status_code == 200:
                                        download_url = image_response.json().get('download_url')
                                        logger.debug(f"download_url: {download_url}")
                                        if USE_OAIUSERCONTENT_URL == True and response_format == "url":
                                            image_link = f"{download_url}"
                                            image_urls.append(image_link)  # å°†å›¾ç‰‡é“¾æ¥ä¿å­˜åˆ°åˆ—è¡¨ä¸­
                                            new_text = ""
                                        else:
                                            if response_format == "url":
                                                # ä»URLä¸‹è½½å›¾ç‰‡
                                                # image_data = requests.get(download_url).content
                                                image_download_response = requests.get(download_url)
                                                # print(f"image_download_response: {image_download_response.text}")
                                                if image_download_response.status_code == 200:
                                                    logger.debug(f"ä¸‹è½½å›¾ç‰‡æˆåŠŸ")
                                                    image_data = image_download_response.content
                                                    today_image_url = save_image(image_data)  # ä¿å­˜å›¾ç‰‡ï¼Œå¹¶è·å–æ–‡ä»¶å
                                                    # new_text = f"\n![image]({UPLOAD_BASE_URL}/{today_image_url})\n[ä¸‹è½½é“¾æ¥]({UPLOAD_BASE_URL}/{today_image_url})\n"
                                                    image_link = f"{UPLOAD_BASE_URL}/{today_image_url}"
                                                    image_urls.append(image_link)  # å°†å›¾ç‰‡é“¾æ¥ä¿å­˜åˆ°åˆ—è¡¨ä¸­
                                                    new_text = ""
                                                else:
                                                    logger.error(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {image_download_response.text}")
                                            else:
                                                # ä½¿ç”¨base64ç¼–ç å›¾ç‰‡
                                                # image_data = requests.get(download_url).content
                                                image_download_response = requests.get(download_url)
                                                if image_download_response.status_code == 200:
                                                    logger.debug(f"ä¸‹è½½å›¾ç‰‡æˆåŠŸ")
                                                    image_data = image_download_response.content
                                                    image_base64 = base64.b64encode(image_data).decode('utf-8')
                                                    image_urls.append(image_base64)
                                                    new_text = ""
                                                else:
                                                    logger.error(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {image_download_response.text}")
                                        if last_content_type == "code":
                                            new_text = new_text
                                            # new_text = "\n```\n" + new_text
                                        logger.debug(f"new_text: {new_text}")
                                        is_img_message = True
                                    else:
                                        logger.error(f"è·å–å›¾ç‰‡ä¸‹è½½é“¾æ¥å¤±è´¥: {image_response.text}")
                            except:
                                pass

                        if is_img_message == False:
                            # print(f"data_json: {data_json}")
                            if content_type == "multimodal_text" and last_content_type == "code":
                                new_text = "\n```\n" + content.get("text", "")
                            elif role == "tool" and name == "dalle.text2im":
                                logger.debug(f"æ— è§†æ¶ˆæ¯: {content.get('text', '')}")
                                continue
                            # ä»£ç å—ç‰¹æ®Šå¤„ç†
                            if content_type == "code" and last_content_type != "code" and content_type != None:
                                full_code = ''.join(content.get("text", ""))
                                new_text = "\n```\n" + full_code[len(last_full_code):]
                                # print(f"full_code: {full_code}")
                                # print(f"last_full_code: {last_full_code}")
                                # print(f"new_text: {new_text}")
                                last_full_code = full_code  # æ›´æ–°å®Œæ•´ä»£ç ä»¥å¤‡ä¸‹æ¬¡æ¯”è¾ƒ

                            elif last_content_type == "code" and content_type != "code" and content_type != None:
                                full_code = ''.join(content.get("text", ""))
                                new_text = "\n```\n" + full_code[len(last_full_code):]
                                # print(f"full_code: {full_code}")
                                # print(f"last_full_code: {last_full_code}")
                                # print(f"new_text: {new_text}")
                                last_full_code = ""  # æ›´æ–°å®Œæ•´ä»£ç ä»¥å¤‡ä¸‹æ¬¡æ¯”è¾ƒ

                            elif content_type == "code" and last_content_type == "code" and content_type != None:
                                full_code = ''.join(content.get("text", ""))
                                new_text = full_code[len(last_full_code):]
                                # print(f"full_code: {full_code}")
                                # print(f"last_full_code: {last_full_code}")
                                # print(f"new_text: {new_text}")
                                last_full_code = full_code  # æ›´æ–°å®Œæ•´ä»£ç ä»¥å¤‡ä¸‹æ¬¡æ¯”è¾ƒ

                            else:
                                # åªè·å–æ–°çš„ parts
                                parts = content.get("parts", [])
                                full_text = ''.join(parts)
                                new_text = full_text[len(last_full_text):]
                                last_full_text = full_text  # æ›´æ–°å®Œæ•´æ–‡æœ¬ä»¥å¤‡ä¸‹æ¬¡æ¯”è¾ƒ
                                if "\u3010" in new_text and not citation_accumulating:
                                    citation_accumulating = True
                                    citation_buffer = citation_buffer + new_text
                                    logger.debug(f"å¼€å§‹ç§¯ç´¯å¼•ç”¨: {citation_buffer}")
                                elif citation_accumulating:
                                    citation_buffer += new_text
                                    logger.debug(f"ç§¯ç´¯å¼•ç”¨: {citation_buffer}")
                                if citation_accumulating:
                                    if is_valid_citation_format(citation_buffer):
                                        logger.debug(f"åˆæ³•æ ¼å¼: {citation_buffer}")
                                        # ç»§ç»­ç§¯ç´¯
                                        if is_complete_citation_format(citation_buffer):

                                            # æ›¿æ¢å®Œæ•´çš„å¼•ç”¨æ ¼å¼
                                            replaced_text, remaining_text, is_potential_citation = replace_complete_citation(
                                                citation_buffer, citations)
                                            # print(replaced_text)  # è¾“å‡ºæ›¿æ¢åçš„æ–‡æœ¬
                                            new_text = replaced_text

                                            if (is_potential_citation):
                                                citation_buffer = remaining_text
                                            else:
                                                citation_accumulating = False
                                                citation_buffer = ""
                                            logger.debug(f"æ›¿æ¢å®Œæ•´çš„å¼•ç”¨æ ¼å¼: {new_text}")
                                        else:
                                            continue
                                    else:
                                        # ä¸æ˜¯åˆæ³•æ ¼å¼ï¼Œæ”¾å¼ƒç§¯ç´¯å¹¶å“åº”
                                        logger.debug(f"ä¸åˆæ³•æ ¼å¼: {citation_buffer}")
                                        new_text = citation_buffer
                                        citation_accumulating = False
                                        citation_buffer = ""

                            # Python å·¥å…·æ‰§è¡Œè¾“å‡ºç‰¹æ®Šå¤„ç†
                            if role == "tool" and name == "python" and last_content_type != "execution_output" and content_type != None:

                                full_code_result = ''.join(content.get("text", ""))
                                new_text = "`Result:` \n```\n" + full_code_result[len(last_full_code_result):]
                                if last_content_type == "code":
                                    new_text = "\n```\n" + new_text
                                # print(f"full_code_result: {full_code_result}")
                                # print(f"last_full_code_result: {last_full_code_result}")
                                # print(f"new_text: {new_text}")
                                last_full_code_result = full_code_result  # æ›´æ–°å®Œæ•´ä»£ç ä»¥å¤‡ä¸‹æ¬¡æ¯”è¾ƒ
                            elif last_content_type == "execution_output" and (
                                    role != "tool" or name != "python") and content_type != None:
                                # new_text = content.get("text", "") + "\n```"
                                full_code_result = ''.join(content.get("text", ""))
                                new_text = full_code_result[len(last_full_code_result):] + "\n```\n"
                                if content_type == "code":
                                    new_text = new_text + "\n```\n"
                                # print(f"full_code_result: {full_code_result}")
                                # print(f"last_full_code_result: {last_full_code_result}")
                                # print(f"new_text: {new_text}")
                                last_full_code_result = ""  # æ›´æ–°å®Œæ•´ä»£ç ä»¥å¤‡ä¸‹æ¬¡æ¯”è¾ƒ
                            elif last_content_type == "execution_output" and role == "tool" and name == "python" and content_type != None:
                                full_code_result = ''.join(content.get("text", ""))
                                new_text = full_code_result[len(last_full_code_result):]
                                # print(f"full_code_result: {full_code_result}")
                                # print(f"last_full_code_result: {last_full_code_result}")
                                # print(f"new_text: {new_text}")
                                last_full_code_result = full_code_result

                        # print(f"æ”¶åˆ°æ•°æ®: {data_json}")
                        # print(f"æ”¶åˆ°çš„å®Œæ•´æ–‡æœ¬: {full_text}")
                        # print(f"ä¸Šæ¬¡æ”¶åˆ°çš„å®Œæ•´æ–‡æœ¬: {last_full_text}")
                        # print(f"æ–°çš„æ–‡æœ¬: {new_text}")

                        # æ›´æ–° last_content_type
                        if content_type != None:
                            last_content_type = content_type if role != "user" else last_content_type

                        new_data = {
                            "id": chat_message_id,
                            "object": "chat.completion.chunk",
                            "created": timestamp,
                            "model": message.get("metadata", {}).get("model_slug"),
                            "choices": [
                                {
                                    "index": 0,
                                    "delta": {
                                        "content": ''.join(new_text)
                                    },
                                    "finish_reason": None
                                }
                            ]
                        }
                        # print(f"Role: {role}")
                        logger.info(f"å‘é€æ¶ˆæ¯: {new_text}")
                        tmp = 'data: ' + json.dumps(new_data, ensure_ascii=False) + '\n\n'
                        # print(f"å‘é€æ•°æ®: {tmp}")
                        # ç´¯ç§¯ new_text
                        all_new_text += new_text
                        yield 'data: ' + json.dumps(new_data, ensure_ascii=False) + '\n\n'
                    except json.JSONDecodeError:
                        # print("JSON è§£æé”™è¯¯")
                        logger.info(f"å‘é€æ•°æ®: {complete_data}")
                        if complete_data == 'data: [DONE]\n\n':
                            logger.info(f"ä¼šè¯ç»“æŸ")
                            yield complete_data
        if citation_buffer != "":
            new_data = {
                "id": chat_message_id,
                "object": "chat.completion.chunk",
                "created": timestamp,
                "model": message.get("metadata", {}).get("model_slug"),
                "choices": [
                    {
                        "index": 0,
                        "delta": {
                            "content": ''.join(citation_buffer)
                        },
                        "finish_reason": None
                    }
                ]
            }
            tmp = 'data: ' + json.dumps(new_data) + '\n\n'
            # print(f"å‘é€æ•°æ®: {tmp}")
            # ç´¯ç§¯ new_text
            all_new_text += citation_buffer
            yield 'data: ' + json.dumps(new_data) + '\n\n'
        if buffer:
            # print(f"æœ€åçš„æ•°æ®: {buffer}")
            # delete_conversation(conversation_id, api_key)
            try:
                buffer_json = json.loads(buffer)
                error_message = buffer_json.get("detail", {}).get("message", "æœªçŸ¥é”™è¯¯")
                error_data = {
                    "id": chat_message_id,
                    "object": "chat.completion.chunk",
                    "created": timestamp,
                    "model": "error",
                    "choices": [
                        {
                            "index": 0,
                            "delta": {
                                "content": ''.join("```\n" + error_message + "\n```")
                            },
                            "finish_reason": None
                        }
                    ]
                }
                tmp = 'data: ' + json.dumps(error_data) + '\n\n'
                logger.info(f"å‘é€æœ€åçš„æ•°æ®: {tmp}")
                # ç´¯ç§¯ new_text
                all_new_text += ''.join("```\n" + error_message + "\n```")
                yield 'data: ' + json.dumps(error_data) + '\n\n'
            except:
                # print("JSON è§£æé”™è¯¯")
                logger.info(f"å‘é€æœ€åçš„æ•°æ®: {buffer}")
                yield buffer

        # delete_conversation(conversation_id, api_key)

    # æ‰§è¡Œæµå¼å“åº”çš„ç”Ÿæˆå‡½æ•°æ¥ç´¯ç§¯ all_new_text
    # è¿­ä»£ç”Ÿæˆå™¨å¯¹è±¡ä»¥æ‰§è¡Œå…¶å†…éƒ¨é€»è¾‘
    for _ in generate(proxy_api_prefix):
        pass
    # æ„é€ å“åº”çš„ JSON ç»“æ„
    response_json = {}
    # æ£€æŸ¥ image_urls æ˜¯å¦ä¸ºç©º
    if not image_urls:
        response_json = {
            "error": {
                "message": all_new_text,  # ä½¿ç”¨ç´¯ç§¯çš„æ–‡æœ¬ä½œä¸ºé”™è¯¯ä¿¡æ¯
                "type": "invalid_request_error",
                "param": "",
                "code": "content_policy_violation"
            }
        }
    else:
        if response_format == "url":
            response_json = {
                "created": int(time.time()),  # ä½¿ç”¨å½“å‰æ—¶é—´æˆ³
                # "reply": all_new_text,  # ä½¿ç”¨ç´¯ç§¯çš„æ–‡æœ¬
                "data": [
                    {
                        "revised_prompt": all_new_text,  # å°†æè¿°æ–‡æœ¬åŠ å…¥æ¯ä¸ªå­—å…¸
                        "url": url
                    } for url in image_urls
                ]  # å°†å›¾ç‰‡é“¾æ¥åˆ—è¡¨è½¬æ¢ä¸ºæ‰€éœ€æ ¼å¼
            }
        else:
            response_json = {
                "created": int(time.time()),  # ä½¿ç”¨å½“å‰æ—¶é—´æˆ³
                # "reply": all_new_text,  # ä½¿ç”¨ç´¯ç§¯çš„æ–‡æœ¬
                "data": [
                    {
                        "revised_prompt": all_new_text,  # å°†æè¿°æ–‡æœ¬åŠ å…¥æ¯ä¸ªå­—å…¸
                        "b64_json": base64
                    } for base64 in image_urls
                ]  # å°†å›¾ç‰‡é“¾æ¥åˆ—è¡¨è½¬æ¢ä¸ºæ‰€éœ€æ ¼å¼
            }
    logger.debug(f"response_json: {response_json}")

    # è¿”å› JSON å“åº”
    return jsonify(response_json)


@app.after_request
def after_request(response):
    response.headers.add('Access-Control-Allow-Origin', '*')
    response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization,X-Requested-With')
    response.headers.add('Access-Control-Allow-Methods', 'GET,PUT,POST,DELETE,OPTIONS')
    return response


# ç‰¹æ®Šçš„ OPTIONS è¯·æ±‚å¤„ç†å™¨
@app.route(f'/{API_PREFIX}/v1/chat/completions' if API_PREFIX else '/v1/chat/completions', methods=['OPTIONS'])
def options_handler():
    logger.info(f"Options Request")
    return Response(status=200)


@app.route('/', defaults={'path': ''}, methods=["GET", "POST", "PUT", "DELETE", "OPTIONS", "HEAD", "PATCH"])
@app.route('/<path:path>', methods=["GET", "POST", "PUT", "DELETE", "OPTIONS", "HEAD", "PATCH"])
def catch_all(path):
    logger.debug(f"æœªçŸ¥è¯·æ±‚: {path}")
    logger.debug(f"è¯·æ±‚æ–¹æ³•: {request.method}")
    logger.debug(f"è¯·æ±‚å¤´: {request.headers}")
    logger.debug(f"è¯·æ±‚ä½“: {request.data}")

    html_string = f"""
        <!DOCTYPE html>
        <html lang="en">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>Document</title>
            </head>
            <body>
                <p> Thanks for using RefreshToV1Api {VERSION}</p>
                <p> æ„Ÿè°¢Ink-Osierå¤§ä½¬çš„ä»˜å‡ºï¼Œæ•¬ç¤¼ï¼ï¼ï¼</p>
                <p><a href="https://github.com/Yanyutin753/RefreshToV1Api">é¡¹ç›®åœ°å€</a></p>
            </body>
        </html>
        """
    return html_string, 500


@app.route('/images/<filename>')
@cross_origin()  # ä½¿ç”¨è£…é¥°å™¨æ¥å…è®¸è·¨åŸŸè¯·æ±‚
def get_image(filename):
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.isfile(os.path.join('images', filename)):
        return "æ–‡ä»¶ä¸å­˜åœ¨å“¦ï¼", 404
    return send_from_directory('images', filename)


@app.route('/files/<filename>')
@cross_origin()  # ä½¿ç”¨è£…é¥°å™¨æ¥å…è®¸è·¨åŸŸè¯·æ±‚
def get_file(filename):
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.isfile(os.path.join('files', filename)):
        return "æ–‡ä»¶ä¸å­˜åœ¨å“¦ï¼", 404
    return send_from_directory('files', filename)


@app.route(f'/{API_PREFIX}/getAccountID' if API_PREFIX else '/getAccountID', methods=['POST'])
@cross_origin()  # ä½¿ç”¨è£…é¥°å™¨æ¥å…è®¸è·¨åŸŸè¯·æ±‚
def getAccountID():
    logger.info(f"New Account Request")
    proxy_api_prefix = getPROXY_API_PREFIX(lock)
    if proxy_api_prefix is None:
        return jsonify({"error": "PROXY_API_PREFIX is not accessible"}), 401
    auth_header = request.headers.get('Authorization')
    if not auth_header or not auth_header.startswith('Bearer '):
        return jsonify({"error": "Authorization header is missing or invalid"}), 401
    api_key = auth_header.split(' ')[1].split(',')[0].strip()

    if not api_key.startswith("eyJhb"):
        refresh_token = api_key
        if api_key in refresh_dict:
            logger.info(f"ä»ç¼“å­˜è¯»å–åˆ°api_key.........")
            api_key = refresh_dict.get(api_key)
        else:
            if REFRESH_TOACCESS_ENABLEOAI:
                api_key = oaiGetAccessToken(api_key)
            else:
                api_key = oaiFreeGetAccessToken(REFRESH_TOACCESS_OAIFREE_REFRESHTOACCESS_URL, api_key)
            if not api_key.startswith("eyJhb"):
                return jsonify({"error": "refresh_token is wrong or refresh_token url is wrong!"}), 401
            add_to_dict(refresh_token, api_key)
    logger.info(f"api_key: {api_key}")

    url = f"{BASE_URL}{proxy_api_prefix}/backend-api/accounts/check/v4-2023-04-27"
    headers = {
        "Authorization": "Bearer " + api_key
    }
    res = requests.get(url, headers=headers)
    if res.status_code == 200:
        data = res.json()
        result = {"plus": set(), "team": set()}
        for account_id, account_data in data["accounts"].items():
            plan_type = account_data["account"]["plan_type"]
            if plan_type == "team":
                result[plan_type].add(account_id)
            elif plan_type == "plus":
                result[plan_type].add(account_id)
        result = {plan_type: list(ids) for plan_type, ids in result.items()}
        return jsonify(result)
    else:
        return jsonify({"error": "Request failed."}), 400


# å†…ç½®è‡ªåŠ¨åˆ·æ–°access_token
def updateRefresh_dict():
    success_num = 0
    error_num = 0
    logger.info(f"==========================================")
    logging.info("å¼€å§‹æ›´æ–°access_token.........")
    for key in refresh_dict:
        refresh_token = key
        if REFRESH_TOACCESS_ENABLEOAI:
            access_token = oaiGetAccessToken(key)
        else:
            access_token = oaiFreeGetAccessToken(REFRESH_TOACCESS_OAIFREE_REFRESHTOACCESS_URL, key)
        if not access_token.startswith("eyJhb"):
            logger.debug("refresh_token is wrong or refresh_token url is wrong!")
            error_num += 1
        add_to_dict(refresh_token, access_token)
        success_num += 1
    logging.info("æ›´æ–°æˆåŠŸ: " + str(success_num) + ", å¤±è´¥: " + str(error_num))
    logger.info(f"==========================================")
    logging.info("å¼€å§‹æ›´æ–°KEY_FOR_GPTS_INFO_ACCESS_TOKENå’ŒGPTSé…ç½®ä¿¡æ¯.......")
    # åŠ è½½é…ç½®å¹¶æ·»åŠ åˆ°å…¨å±€åˆ—è¡¨
    gpts_data = load_gpts_config("./data/gpts.json")
    add_config_to_global_list(BASE_URL, getPROXY_API_PREFIX(lock), gpts_data)

    accessible_model_list = get_accessible_model_list()
    logger.info(f"å½“å‰å¯ç”¨ GPTS åˆ—è¡¨: {accessible_model_list}")

    # æ£€æŸ¥åˆ—è¡¨ä¸­æ˜¯å¦æœ‰é‡å¤çš„æ¨¡å‹åç§°
    if len(accessible_model_list) != len(set(accessible_model_list)):
        raise Exception("æ£€æµ‹åˆ°é‡å¤çš„æ¨¡å‹åç§°ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶......")
    logging.info("æ›´æ–°KEY_FOR_GPTS_INFO_ACCESS_TOKENå’ŒGPTSé…ç½®ä¿¡æ¯æˆåŠŸ......")
    logger.info(f"==========================================")


# æ¯å¤©3ç‚¹è‡ªåŠ¨åˆ·æ–°
scheduler.add_job(id='updateRefresh_run', func=updateRefresh_dict, trigger='cron', hour=3, minute=0)

# è¿è¡Œ Flask åº”ç”¨
if __name__ == '__main__':
    app.run(host='0.0.0.0', port=33333, threaded=True)
